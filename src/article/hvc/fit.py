from pathlib import Path

import joblib
import pandas as pd

from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.utils.fixes import loguniform
# need to import this for HalvingRandomSearchCV to work!!!
from sklearn.experimental import enable_halving_search_cv
from sklearn.model_selection import HalvingRandomSearchCV
from sklearn.svm import SVC

from tqdm import tqdm


def fit(extract_csv_path,
        learncurve_csv_path,
        split='train'):
    """fit an SVM to features extracted
    by ``article.hvc.extract.extract``.

    Parameters
    ----------
    extract_csv_path : str, pathlib.Path
        path to .csv saved by ``article.hvc.extract``
    learncurve_csv_path : str, pathlib.Path
        path to original .csv generated by ``vak prep``,
         that was used to create the ``extract`` csv
    split : str
        split to use to fit models,
        default is 'train'

    Returns
    -------
    clf : sklearn.SVM.SVC
        instance, fit by pipeline

    Notes
    -----
    Performs an inner merge with
    the .csv saved by the ``extract``
    function and the original .csv saved by
    ``vak prep``, so that feature file paths in
    the ``extract`` csv are aligned with
    defined dataset splits in the ``prep`` csv.

    Then trains SVM using the specified split.

    Training is done with a ``sklearn`` pipeline
    that makes use of random search
    for hyperparameters, with halving.
    """
    extract_df = pd.read_csv(extract_csv_path)
    learncurve_df = pd.read_csv(learncurve_csv_path)

    # do an INNER JOIN to add 'features_path' from DataFrame saved by extract script
    # to this split of the dataset generated by `learncurve`
    learncurve_df = pd.merge(learncurve_df, extract_df, how='inner')

    ftr_paths = learncurve_df[learncurve_df.split == split].features_path.values.tolist()
    df = []
    for ftr_path in tqdm(ftr_paths):
        df.append(
            pd.read_csv(ftr_path)
        )
    df = pd.concat(df)
    y = df.labels.values
    x = df[df.columns.drop('labels')].values

    tuned_parameters = {
        'svc__kernel': ['rbf'],
        'svc__gamma': loguniform(1e-9, 1e13),
        'svc__C': loguniform(1e-2, 1e10)
    }

    pipe = make_pipeline(StandardScaler(), SVC())
    clf = HalvingRandomSearchCV(
        pipe, tuned_parameters, factor=3, n_jobs=-2, verbose=1,
    )
    clf.fit(x, y)

    return clf


def save_clf(clf,
             csv_path,
             clf_dst):
    """save trained scikit-learn
    classifier, using ``joblib``

    Parameters
    ----------
    clf :
    csv_path : str, pathlib.Path
        path to csv created by ``extract``
        with features used to train classifier.
        Associates classifier with training set
        via the filename.
    clf_dst : str, pathlib.Path
        directory where ``joblib`` should save
        binary blob of classifier

    Returns
    -------
    clf_path : pathlib.Path
        path to saved classifier.
        Used when running ``article.hvc.predict.predict``.
    """
    csv_path = Path(csv_path)
    clf_fname = f'{csv_path.name}.svm.joblib'
    clf_path = clf_dst / clf_fname
    joblib.dump(clf, clf_path)
    return clf_path
