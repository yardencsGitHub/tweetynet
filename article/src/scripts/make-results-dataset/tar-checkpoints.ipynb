{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c728d87-9498-447a-afe7-bcfb068640d0",
   "metadata": {},
   "source": [
    "This notebooks makes the .tar.gz files (compressed archives) with checkpoints that are in the Dryad \"results\" dataset that accompanies the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcb01cdc-7d67-4708-8100-618902fab26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import tarfile\n",
    "\n",
    "import pandas as pd\n",
    "import pyprojroot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef74e7ba-d98c-45d8-8737-22a77a95fa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_root = pyprojroot.here() / 'results'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3642ecbf-f16e-4814-8191-dcb2417cf7ac",
   "metadata": {},
   "source": [
    "Use error .csv files to figure out which results dirs we need to tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9e541c1-a57b-4702-a29e-f6f172b868b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "err_csvs = sorted(results_root.rglob('err*csv'))\n",
    "err_csvs = [\n",
    "    err_csv\n",
    "    for err_csv in err_csvs\n",
    "    if 'Bengalese_Finches' in str(err_csv) or 'Canaries' in str(err_csv)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7de8cf69-67f0-4900-8486-d4c4ca73b52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "these_expt_dirs = [\n",
    "    'learncurve',\n",
    "    'long_train',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60d9f1df-6b70-433d-8109-cda4135edbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "err_csvs = [\n",
    "    err_csv\n",
    "    for err_csv in err_csvs\n",
    "    if any([expt_dir in str(err_csv) for expt_dir in these_expt_dirs]) and not '.ipynb_checkpoints' in str(err_csv)\n",
    "]           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b6c1cb3e-8173-43d2-9285-3c74c6a2615a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dirs = []\n",
    "for err_csv in err_csvs:\n",
    "    df = pd.read_csv(err_csv)\n",
    "    if 'results_dir' in df:\n",
    "        results_dirs.extend(\n",
    "            df['results_dir'].unique().tolist()\n",
    "        )\n",
    "    else:\n",
    "        assert 'long_train' in str(err_csv)\n",
    "        results_root = err_csv.parent\n",
    "        subdirs = [subdir for subdir in results_root.iterdir() if subdir.is_dir() and subdir.name.startswith('ll')]\n",
    "        subdir_results_dirs = []\n",
    "        for subdir in subdirs:\n",
    "            subdir_results_dir = sorted(subdir.glob('results_*'))\n",
    "            assert len(subdir_results_dir) == 1\n",
    "            subdir_results_dir = subdir_results_dir[0]\n",
    "            subdir_results_dirs.append(subdir_results_dir)\n",
    "        subdir_results_dirs = [str(results_dir).replace('/home/art/Documents/repos/coding/birdsong/tweetynet/article/', '') \n",
    "                               for results_dir in subdir_results_dirs]\n",
    "        results_dirs.extend(subdir_results_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ac1c217-0f2b-40a2-8883-7c01b9d349cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "\n",
    "# add it to the root logger\n",
    "logging.getLogger().addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d4778c3-a54e-4deb-9c52-b87514c1e11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Making tar from:\n",
      "/home/art/Documents/repos/coding/birdsong/tweetynet/article/results/Bengalese_Finches/learncurve/Bird0/results_210528_225043\n",
      "found 10 checkpoint files for largest training set duration\n",
      "will generate archive as: /home/art/Documents/repos/coding/birdsong/tweetynet/article/results/tars/Bengalese_Finches-learncurve-Bird0-checkpoints.tar.gz\n",
      "\n",
      "Making tar from:\n",
      "/home/art/Documents/repos/coding/birdsong/tweetynet/article/results/Bengalese_Finches/learncurve/Bird4/results_210529_031959\n",
      "found 10 checkpoint files for largest training set duration\n",
      "will generate archive as: /home/art/Documents/repos/coding/birdsong/tweetynet/article/results/tars/Bengalese_Finches-learncurve-Bird4-checkpoints.tar.gz\n",
      "\n",
      "Making tar from:\n",
      "/home/art/Documents/repos/coding/birdsong/tweetynet/article/results/Bengalese_Finches/learncurve/Bird7/results_210527_213421\n",
      "found 10 checkpoint files for largest training set duration\n",
      "will generate archive as: /home/art/Documents/repos/coding/birdsong/tweetynet/article/results/tars/Bengalese_Finches-learncurve-Bird7-checkpoints.tar.gz\n",
      "\n",
      "Making tar from:\n",
      "/home/art/Documents/repos/coding/birdsong/tweetynet/article/results/Bengalese_Finches/learncurve/Bird9/results_210528_024923\n",
      "found 10 checkpoint files for largest training set duration\n",
      "will generate archive as: /home/art/Documents/repos/coding/birdsong/tweetynet/article/results/tars/Bengalese_Finches-learncurve-Bird9-checkpoints.tar.gz\n",
      "\n",
      "Making tar from:\n",
      "/home/art/Documents/repos/coding/birdsong/tweetynet/article/results/Bengalese_Finches/learncurve/bl26lb16/results_210509_153037\n",
      "found 10 checkpoint files for largest training set duration\n",
      "will generate archive as: /home/art/Documents/repos/coding/birdsong/tweetynet/article/results/tars/Bengalese_Finches-learncurve-bl26lb16-checkpoints.tar.gz\n",
      "\n",
      "Making tar from:\n",
      "/home/art/Documents/repos/coding/birdsong/tweetynet/article/results/Bengalese_Finches/learncurve/gr41rd51/results_210521_213108\n",
      "found 10 checkpoint files for largest training set duration\n",
      "will generate archive as: /home/art/Documents/repos/coding/birdsong/tweetynet/article/results/tars/Bengalese_Finches-learncurve-gr41rd51-checkpoints.tar.gz\n",
      "\n",
      "Making tar from:\n",
      "/home/art/Documents/repos/coding/birdsong/tweetynet/article/results/Bengalese_Finches/learncurve/gy6or6/results_210509_010443\n",
      "found 10 checkpoint files for largest training set duration\n",
      "will generate archive as: /home/art/Documents/repos/coding/birdsong/tweetynet/article/results/tars/Bengalese_Finches-learncurve-gy6or6-checkpoints.tar.gz\n",
      "\n",
      "Making tar from:\n",
      "/home/art/Documents/repos/coding/birdsong/tweetynet/article/results/Bengalese_Finches/learncurve/or60yw70/results_210509_065128\n",
      "found 10 checkpoint files for largest training set duration\n",
      "will generate archive as: /home/art/Documents/repos/coding/birdsong/tweetynet/article/results/tars/Bengalese_Finches-learncurve-or60yw70-checkpoints.tar.gz\n",
      "\n",
      "Making tar from:\n",
      "/home/art/Documents/repos/coding/birdsong/tweetynet/article/results/Canaries/learncurve/llb11/results_200610_101801\n",
      "found 7 checkpoint files for largest training set duration\n",
      "will generate archive as: /home/art/Documents/repos/coding/birdsong/tweetynet/article/results/tars/Canaries-learncurve-llb11-checkpoints.tar.gz\n",
      "\n",
      "Making tar from:\n",
      "/home/art/Documents/repos/coding/birdsong/tweetynet/article/results/Canaries/learncurve/llb16/results_200613_145015\n",
      "found 7 checkpoint files for largest training set duration\n",
      "will generate archive as: /home/art/Documents/repos/coding/birdsong/tweetynet/article/results/tars/Canaries-learncurve-llb16-checkpoints.tar.gz\n",
      "\n",
      "Making tar from:\n",
      "/home/art/Documents/repos/coding/birdsong/tweetynet/article/results/Canaries/learncurve/llb3/results_200605_104920\n",
      "found 7 checkpoint files for largest training set duration\n",
      "will generate archive as: /home/art/Documents/repos/coding/birdsong/tweetynet/article/results/tars/Canaries-learncurve-llb3-checkpoints.tar.gz\n",
      "\n",
      "Making tar from:\n",
      "/home/art/Documents/repos/coding/birdsong/tweetynet/article/results/Canaries/long_train/long_train/llb16/results_210529_114433\n",
      "found 1 checkpoint files for largest training set duration\n",
      "will generate archive as: /home/art/Documents/repos/coding/birdsong/tweetynet/article/results/tars/Canaries-long_train-long_train-llb16-checkpoints.tar.gz\n",
      "\n",
      "Making tar from:\n",
      "/home/art/Documents/repos/coding/birdsong/tweetynet/article/results/Canaries/long_train/long_train/llb11/results_210529_112150\n",
      "found 1 checkpoint files for largest training set duration\n",
      "will generate archive as: /home/art/Documents/repos/coding/birdsong/tweetynet/article/results/tars/Canaries-long_train-long_train-llb11-checkpoints.tar.gz\n",
      "\n",
      "Making tar from:\n",
      "/home/art/Documents/repos/coding/birdsong/tweetynet/article/results/Canaries/long_train/long_train/llb3/results_210528_224118\n",
      "found 1 checkpoint files for largest training set duration\n",
      "will generate archive as: /home/art/Documents/repos/coding/birdsong/tweetynet/article/results/tars/Canaries-long_train-long_train-llb3-checkpoints.tar.gz\n"
     ]
    }
   ],
   "source": [
    "DRY_RUN = False\n",
    "SKIP_EXISTING_TAR = False\n",
    "\n",
    "TAR_ROOT_PATH = pyprojroot.here() / 'results' / 'tars'\n",
    "\n",
    "logger = logging.Logger('targz.logger', level=logging.DEBUG)\n",
    "\n",
    "for results_dir in results_dirs:\n",
    "    # need to fix path for window_size_352, it's different for some reason\n",
    "    if not results_dir.startswith('results') and results_dir.startswith(\n",
    "        '/home/art/Documents/repos/coding/birdsong/tweetynet/'\n",
    "    ):\n",
    "        results_dir = results_dir.replace('/home/art/Documents/repos/coding/birdsong/tweetynet/', '')\n",
    "\n",
    "    results_dir_path = pyprojroot.here() / results_dir\n",
    "\n",
    "    if not results_dir_path.exists():\n",
    "        print(\n",
    "            f'does not exist: {results_dir_path}'\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            f'\\nMaking tar from:\\n{results_dir_path}'\n",
    "        )\n",
    "\n",
    "        if 'long_train' in results_dir:\n",
    "            # only one checkpoint but use glob as a hack to get this to work the same way for long_train and learncurve\n",
    "            checkpoints = sorted(results_dir_path.glob('TweetyNet/checkpoints/max-val-acc-checkpoint.pt'))\n",
    "        else:\n",
    "            largest_train_dur_dir = sorted(results_dir_path.glob('train_dur_*'), \n",
    "               key=lambda x: int(x.name.split('_')[-1].replace('s', ''))\n",
    "              )[-1]\n",
    "            checkpoints = sorted(largest_train_dur_dir.glob('replicate_*/TweetyNet/checkpoints/max-val-acc-checkpoint.pt'))\n",
    "        print(\n",
    "            f'found {len(checkpoints)} checkpoint files for largest training set duration'\n",
    "        )\n",
    "\n",
    "        tar_name = '-'.join(results_dir.split('/')[1:-1])\n",
    "        tar_path = TAR_ROOT_PATH / f'{tar_name}-checkpoints.tar.gz'\n",
    "        print(\n",
    "            f'will generate archive as: {tar_path}'\n",
    "        )\n",
    "\n",
    "        if SKIP_EXISTING_TAR:\n",
    "            if tar_path.exists():\n",
    "                print (\n",
    "                    f'\\tSKIP_EXISTING_TAR is true and tar exists:\\n\\t{tar_path}.\\n\\tSkipping.'\n",
    "                )\n",
    "\n",
    "        if not DRY_RUN:\n",
    "            tar = tarfile.open(str(tar_path), \"w|gz\")\n",
    "            for checkpoint in checkpoints:\n",
    "                tar.add(checkpoint)\n",
    "            tar.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
