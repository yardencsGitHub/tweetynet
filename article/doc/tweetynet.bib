
@article{abdel-hamidConvolutionalNeuralNetworks2014,
  title = {Convolutional {{Neural Networks}} for {{Speech Recognition}}},
  author = {{Abdel-Hamid}, Ossama and Mohamed, Abdel-rahman and Jiang, Hui and Deng, Li and Penn, Gerald and Yu, Dong},
  year = {2014},
  month = oct,
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {22},
  number = {10},
  pages = {1533--1545},
  issn = {2329-9290, 2329-9304},
  doi = {10.1109/TASLP.2014.2339736},
  abstract = {Recently, the hybrid deep neural network (DNN)hidden Markov model (HMM) has been shown to significantly improve speech recognition performance over the conventional Gaussian mixture model (GMM)-HMM. The performance improvement is partially attributed to the ability of the DNN to model complex correlations in speech features. In this paper, we show that further error rate reduction can be obtained by using convolutional neural networks (CNNs). We first present a concise description of the basic CNN and explain how it can be used for speech recognition. We further propose a limited-weight-sharing scheme that can better model speech features. The special structure such as local connectivity, weight sharing, and pooling in CNNs exhibits some degree of invariance to small shifts of speech features along the frequency axis, which is important to deal with speaker and environment variations. Experimental results show that CNNs reduce the error rate by 6\%-10\% compared with DNNs on the TIMIT phone recognition and the voice search large vocabulary speech recognition tasks.},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/9K5MZKK9/Abdel-Hamid et al. - 2014 - Convolutional Neural Networks for Speech Recogniti.pdf}
}

@article{abdel2014convolutional,
  title = {Convolutional Neural Networks for Speech Recognition},
  author = {{Abdel-Hamid}, Ossama and Mohamed, Abdel-rahman and Jiang, Hui and Deng, Li and Penn, Gerald and Yu, Dong},
  year = {2014},
  journal = {IEEE/ACM Transactions on audio, speech, and language processing},
  volume = {22},
  number = {10},
  pages = {1533--1545},
  publisher = {{IEEE}}
}

@article{alliende_seasonal_2017,
  title = {Seasonal Plasticity of Song Behavior Relies on Motor and Syntactic Variability Induced by a Basal Ganglia\textendash Forebrain Circuit},
  author = {Alliende, Jorge and Giret, Nicolas and Pidoux, Ludivine and Del Negro, Catherine and Leblois, Arthur},
  year = {2017},
  month = sep,
  journal = {Neuroscience},
  volume = {359},
  pages = {49--68},
  issn = {0306-4522},
  doi = {10.1016/j.neuroscience.2017.07.007},
  abstract = {The plasticity of nervous systems allows animals to quickly adapt to a changing environment. In particular, seasonal plasticity of brain structure and behavior is often critical to survival or mating in seasonal climates. Songbirds provide striking examples of seasonal changes in neural circuits and vocal behavior and have emerged as a leading model for adult brain plasticity. While seasonal plasticity and the well-characterized process of juvenile song learning may share common neural mechanisms, the extent of their similarity remains unclear. Especially, it is unknown whether the basal ganglia (BG)\textendash forebrain loop which implements song learning in juveniles by driving vocal exploration participates in seasonal plasticity. To address this issue, we performed bilateral lesions of the output structure of the song-related BG\textendash forebrain circuit (the magnocellular nucleus of the anterior nidopallium) in canaries during the breeding season, when song is most stereotyped, and just after resuming singing in early fall, when canaries sing their most variable songs and may produce new syllable types. Lesions drastically reduced song acoustic variability, increased song and phrase duration, and decreased syntax variability in early fall, reverting at least partially seasonal changes observed between the breeding season and early fall. On the contrary, lesions did not affect singing behavior during the breeding season. Our results therefore indicate that the BG\textendash forebrain pathway introduces acoustic and syntactic variability in song when canaries resume singing in early fall. We propose that BG\textendash forebrain circuits actively participate in seasonal plasticity by injecting variability in behavior during non-breeding season. Significance Statement The study of seasonal plasticity in temperate songbirds has provided important insights into the mechanisms of structural and functional plasticity in the central nervous system. The precise function and mechanisms of seasonal song plasticity however remain poorly understood. We show here that a basal ganglia\textendash forebrain circuit involved in the acquisition and maintenance of birdsong is actively inducing song variability outside the breeding season, when singing is most variable, while having little effect on the stereotyped singing during the breeding season. Our results suggest that seasonal plasticity reflects an active song-maintenance process akin to juvenile learning, and that basal ganglia\textendash forebrain circuits can drive plasticity in a learned vocal behavior during the non\textendash injury-induced degeneration and reconstruction of the neural circuit underlying its production.},
  langid = {english},
  keywords = {basal ganglia,birdsong,canary,seasonal plasticity,variability}
}

@article{alliende_species-specific_2013,
  title = {A Species-Specific View of Song Representation in a Sensorimotor Nucleus},
  author = {Alliende, Jorge and Lehongre, Katia and Del Negro, Catherine},
  year = {2013},
  month = jun,
  journal = {Journal of Physiology-Paris},
  series = {Special Issue: {{Physiological}} Mechanisms of Song Learning and Production},
  volume = {107},
  number = {3},
  pages = {193--202},
  issn = {0928-4257},
  doi = {10.1016/j.jphysparis.2012.08.004},
  abstract = {Songbirds constitute a powerful model system for the investigation of how complex vocal communication sounds are represented and generated, offering a neural system in which the brain areas involved in auditory, motor and auditory\textendash motor integration are well known. One brain area of considerable interest is the nucleus HVC. Neurons in the HVC respond vigorously to the presentation of the bird's own song and display song-related motor activity. In the present paper, we present a synthesis of neurophysiological studies performed in the HVC of one songbird species, the canary (Serinus canaria). These studies, by taking advantage of the singing behavior and song characteristics of the canary, have examined the neuronal representation of the bird's own song in the HVC. They suggest that breeding cues influence the degree of auditory selectivity of HVC neurons for the bird's own song over its time-reversed version, without affecting the contribution of spike timing to the information carried by these two song stimuli. Also, while HVC neurons are collectively more responsive to forward playback of the bird's own song than to its temporally or spectrally modified versions, some are more broadly tuned, with an auditory responsiveness that extends beyond the bird's own song. Lastly, because the HVC is also involved in song production, we discuss the peripheral control of song production, and suggest that interspecific variations in song production mechanisms could be exploited to improve our understanding of the functional role of the HVC in respiratory\textendash vocal coordination.},
  langid = {english}
}

@article{alliendeSeasonalPlasticitySong2017,
  title = {Seasonal Plasticity of Song Behavior Relies on Motor and Syntactic Variability Induced by a Basal Ganglia\textendash Forebrain Circuit},
  author = {Alliende, Jorge and Giret, Nicolas and Pidoux, Ludivine and Del Negro, Catherine and Leblois, Arthur},
  year = {2017},
  month = sep,
  journal = {Neuroscience},
  volume = {359},
  pages = {49--68},
  issn = {0306-4522},
  doi = {10.1016/j.neuroscience.2017.07.007},
  abstract = {The plasticity of nervous systems allows animals to quickly adapt to a changing environment. In particular, seasonal plasticity of brain structure and behavior is often critical to survival or mating in seasonal climates. Songbirds provide striking examples of seasonal changes in neural circuits and vocal behavior and have emerged as a leading model for adult brain plasticity. While seasonal plasticity and the well-characterized process of juvenile song learning may share common neural mechanisms, the extent of their similarity remains unclear. Especially, it is unknown whether the basal ganglia (BG)\textendash forebrain loop which implements song learning in juveniles by driving vocal exploration participates in seasonal plasticity. To address this issue, we performed bilateral lesions of the output structure of the song-related BG\textendash forebrain circuit (the magnocellular nucleus of the anterior nidopallium) in canaries during the breeding season, when song is most stereotyped, and just after resuming singing in early fall, when canaries sing their most variable songs and may produce new syllable types. Lesions drastically reduced song acoustic variability, increased song and phrase duration, and decreased syntax variability in early fall, reverting at least partially seasonal changes observed between the breeding season and early fall. On the contrary, lesions did not affect singing behavior during the breeding season. Our results therefore indicate that the BG\textendash forebrain pathway introduces acoustic and syntactic variability in song when canaries resume singing in early fall. We propose that BG\textendash forebrain circuits actively participate in seasonal plasticity by injecting variability in behavior during non-breeding season. Significance Statement The study of seasonal plasticity in temperate songbirds has provided important insights into the mechanisms of structural and functional plasticity in the central nervous system. The precise function and mechanisms of seasonal song plasticity however remain poorly understood. We show here that a basal ganglia\textendash forebrain circuit involved in the acquisition and maintenance of birdsong is actively inducing song variability outside the breeding season, when singing is most variable, while having little effect on the stereotyped singing during the breeding season. Our results suggest that seasonal plasticity reflects an active song-maintenance process akin to juvenile learning, and that basal ganglia\textendash forebrain circuits can drive plasticity in a learned vocal behavior during the non\textendash injury-induced degeneration and reconstruction of the neural circuit underlying its production.},
  langid = {english},
  keywords = {basal ganglia,birdsong,canary,seasonal plasticity,variability}
}

@article{alliendeSpeciesspecificViewSong2013,
  title = {A Species-Specific View of Song Representation in a Sensorimotor Nucleus},
  author = {Alliende, Jorge and Lehongre, Katia and Del Negro, Catherine},
  year = {2013},
  month = jun,
  journal = {Journal of Physiology-Paris},
  series = {Special Issue: {{Physiological}} Mechanisms of Song Learning and Production},
  volume = {107},
  number = {3},
  pages = {193--202},
  issn = {0928-4257},
  doi = {10.1016/j.jphysparis.2012.08.004},
  abstract = {Songbirds constitute a powerful model system for the investigation of how complex vocal communication sounds are represented and generated, offering a neural system in which the brain areas involved in auditory, motor and auditory\textendash motor integration are well known. One brain area of considerable interest is the nucleus HVC. Neurons in the HVC respond vigorously to the presentation of the bird's own song and display song-related motor activity. In the present paper, we present a synthesis of neurophysiological studies performed in the HVC of one songbird species, the canary (Serinus canaria). These studies, by taking advantage of the singing behavior and song characteristics of the canary, have examined the neuronal representation of the bird's own song in the HVC. They suggest that breeding cues influence the degree of auditory selectivity of HVC neurons for the bird's own song over its time-reversed version, without affecting the contribution of spike timing to the information carried by these two song stimuli. Also, while HVC neurons are collectively more responsive to forward playback of the bird's own song than to its temporally or spectrally modified versions, some are more broadly tuned, with an auditory responsiveness that extends beyond the bird's own song. Lastly, because the HVC is also involved in song production, we discuss the peripheral control of song production, and suggest that interspecific variations in song production mechanisms could be exploited to improve our understanding of the functional role of the HVC in respiratory\textendash vocal coordination.},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/JZTRPEL7/Alliende et al. - 2013 - A species-specific view of song representation in .pdf;/Users/davidnicholson/Zotero/storage/T5BA4FY4/S0928425712000447.html}
}

@article{alonso_low-dimensional_2009,
  title = {Low-Dimensional Dynamical Model for the Diversity of Pressure Patterns Used in Canary Song},
  author = {Alonso, Leandro M. and Alliende, Jorge A. and Goller, F. and Mindlin, Gabriel B.},
  year = {2009},
  month = apr,
  journal = {Physical Review E},
  volume = {79},
  number = {4},
  pages = {041929},
  doi = {10.1103/PhysRevE.79.041929},
  abstract = {During song production, oscine birds produce large air sac pressure pulses. During those pulses, energy is transferred to labia located at the juncture between the bronchii and the trachea, inducing the high frequency labial oscillations which are responsible for airflow modulations, i.e., the uttered sound. In order to generate diverse syllables, canaries (Serinus canaria) use a set of air sac pressure patterns with characteristic shapes. In this work we show that these different shapes can be approximated by the subharmonic solutions of a forced normal form. This simple model is built from identifying dynamical elements which allow to reproduce the shape of the pressure pattern corresponding to one syllable type. Remarkably, integrating that simple model for other parameters allows to recover the other pressure patterns used during song. Interpreting the diversity of these physiological gestures as subharmonic solutions of a simple nonlinear system allows us to account simultaneously for their morphological features as well as for the syllabic timing and suggests a strategy for the generation of complex motor patterns.}
}

@article{alonsoLowdimensionalDynamicalModel2009,
  title = {Low-Dimensional Dynamical Model for the Diversity of Pressure Patterns Used in Canary Song},
  author = {Alonso, Leandro M. and Alliende, Jorge A. and Goller, F. and Mindlin, Gabriel B.},
  year = {2009},
  month = apr,
  journal = {Physical Review E},
  volume = {79},
  number = {4},
  pages = {041929},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevE.79.041929},
  abstract = {During song production, oscine birds produce large air sac pressure pulses. During those pulses, energy is transferred to labia located at the juncture between the bronchii and the trachea, inducing the high frequency labial oscillations which are responsible for airflow modulations, i.e., the uttered sound. In order to generate diverse syllables, canaries (Serinus canaria) use a set of air sac pressure patterns with characteristic shapes. In this work we show that these different shapes can be approximated by the subharmonic solutions of a forced normal form. This simple model is built from identifying dynamical elements which allow to reproduce the shape of the pressure pattern corresponding to one syllable type. Remarkably, integrating that simple model for other parameters allows to recover the other pressure patterns used during song. Interpreting the diversity of these physiological gestures as subharmonic solutions of a simple nonlinear system allows us to account simultaneously for their morphological features as well as for the syllabic timing and suggests a strategy for the generation of complex motor patterns.},
  file = {/Users/davidnicholson/Zotero/storage/NUJ3MSWC/Alonso et al. - 2009 - Low-dimensional dynamical model for the diversity .pdf;/Users/davidnicholson/Zotero/storage/K84BPZRM/PhysRevE.79.html}
}

@article{alvarez-buyllaBirthProjectionNeurons1990,
  title = {Birth of {{Projection Neurons}} in {{Adult Avian Brain May Be Related}} to {{Perceptual}} or {{Motor Learning}}},
  author = {{Alvarez-Buylla}, Arturo and Kirn, John R. and Nottebohm, Fernando},
  year = {1990},
  journal = {Science},
  volume = {249},
  number = {4975},
  pages = {1444--1446},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075},
  abstract = {Projection neurons that form part of the motor pathway for song control continue to be produced and to replace older projection neurons in adult canaries and zebra finches. This is shown by combining [\$\^3\$H]thymidine, a cell birth marker, and fluorogold, a retrogradely transported tracer of neuronal connectivity. Species and seasonal comparisons suggest that this process is related to the acquisition of perceptual or motor memories. The ability of an adult brain to produce and replace projection neurons should influence our thinking on brain repair.},
  file = {/Users/davidnicholson/Zotero/storage/HHZHMAKA/Alvarez-Buylla et al. - 1990 - Birth of Projection Neurons in Adult Avian Brain M.pdf}
}

@inproceedings{amodei2016deep,
  title = {Deep Speech 2: {{End-to-end}} Speech Recognition in English and Mandarin},
  booktitle = {International Conference on Machine Learning},
  author = {Amodei, Dario and Ananthanarayanan, Sundaram and Anubhai, Rishita and Bai, Jingliang and Battenberg, Eric and Case, Carl and Casper, Jared and Catanzaro, Bryan and Cheng, Qiang and Chen, Guoliang and others},
  year = {2016},
  pages = {173--182},
  organization = {{PMLR}}
}

@article{amodeiDeepSpeechEndtoEnd2015,
  title = {Deep {{Speech}} 2: {{End-to-End Speech Recognition}} in {{English}} and {{Mandarin}}},
  shorttitle = {Deep {{Speech}} 2},
  author = {Amodei, Dario and Anubhai, Rishita and Battenberg, Eric and Case, Carl and Casper, Jared and Catanzaro, Bryan and Chen, Jingdong and Chrzanowski, Mike and Coates, Adam and Diamos, Greg and Elsen, Erich and Engel, Jesse and Fan, Linxi and Fougner, Christopher and Han, Tony and Hannun, Awni and Jun, Billy and LeGresley, Patrick and Lin, Libby and Narang, Sharan and Ng, Andrew and Ozair, Sherjil and Prenger, Ryan and Raiman, Jonathan and Satheesh, Sanjeev and Seetapun, David and Sengupta, Shubho and Wang, Yi and Wang, Zhiqian and Wang, Chong and Xiao, Bo and Yogatama, Dani and Zhan, Jun and Zhu, Zhenyao},
  year = {2015},
  month = dec,
  journal = {arXiv:1512.02595 [cs]},
  eprint = {1512.02595},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech\textemdash two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, resulting in a 7x speedup over our previous system [26]. Because of this efficiency, experiments that previously took weeks now run in days. This enables us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/davidnicholson/Zotero/storage/3EJV4AGA/Amodei et al. - 2015 - Deep Speech 2 End-to-End Speech Recognition in En.pdf}
}

@article{anderson1996template,
  title = {Template-Based Automatic Recognition of Birdsong Syllables from Continuous Recordings},
  author = {Anderson, Sven E and Dave, Amish S and Margoliash, Daniel},
  year = {1996},
  journal = {The Journal of the Acoustical Society of America},
  volume = {100},
  number = {2},
  pages = {1209--1219},
  publisher = {{Acoustical Society of America}}
}

@article{appeltants_effect_2005,
  title = {The Effect of Auditory Distractors on Song Discrimination in Male Canaries ({{Serinus}} Canaria)},
  author = {Appeltants, Didier and Gentner, Timothy Q. and Hulse, Stewart H. and Balthazart, Jacques and Ball, Gregory F.},
  year = {2005},
  month = jun,
  journal = {Behavioural Processes},
  volume = {69},
  number = {3},
  pages = {331--341},
  issn = {0376-6357},
  doi = {10.1016/j.beproc.2005.01.010},
  abstract = {Male songbirds such as canaries produce complex learned vocalizations that are used in the context of mate attraction and territory defense. Successful mate attraction or territorial defense requires that a bird be able to recognize individuals based on their vocal performance and identify these songs in a noisy background. In order to learn more about how birds are able to solve this problem, we investigated, with a two-alternative choice procedure, the ability of adult male canaries to discriminate between conspecific song segments from two different birds and to maintain this discrimination when conspecific songs are superimposed with a variety of distractors. The results indicate that male canaries have the ability to discriminate, with a high level of accuracy song segments produced by two different conspecific birds. Song discrimination was partially maintained when the stimuli were masked by auditory distractors, but the accuracy of the discrimination progressively declined as a function of the number of masking distractors. The type of distractor used in the experiments (other conspecific songs or different types of artificial white noise) did not markedly affect the rate of deterioration of the song discrimination. These data indicate that adult male canaries have the perceptual abilities to discriminate and selectively attend to one ongoing sound that occurs simultaneously with one or more other sounds. The administration of a noradrenergic neurotoxin did not impair markedly the discrimination learning abilities although the number of subjects tested was too small to allow any firm conclusion. In these conditions, however, the noradrenergic lesion significantly increased the number failures to respond in the discrimination learning task suggesting a role, in canaries, of the noradrenergic system in some attentional processes underlying song learning and processing.},
  langid = {english}
}

@article{appeltantsEffectAuditoryDistractors2005,
  title = {The Effect of Auditory Distractors on Song Discrimination in Male Canaries ({{Serinus}} Canaria)},
  author = {Appeltants, Didier and Gentner, Timothy Q. and Hulse, Stewart H. and Balthazart, Jacques and Ball, Gregory F.},
  year = {2005},
  month = jun,
  journal = {Behavioural Processes},
  volume = {69},
  number = {3},
  pages = {331--341},
  issn = {0376-6357},
  doi = {10.1016/j.beproc.2005.01.010},
  abstract = {Male songbirds such as canaries produce complex learned vocalizations that are used in the context of mate attraction and territory defense. Successful mate attraction or territorial defense requires that a bird be able to recognize individuals based on their vocal performance and identify these songs in a noisy background. In order to learn more about how birds are able to solve this problem, we investigated, with a two-alternative choice procedure, the ability of adult male canaries to discriminate between conspecific song segments from two different birds and to maintain this discrimination when conspecific songs are superimposed with a variety of distractors. The results indicate that male canaries have the ability to discriminate, with a high level of accuracy song segments produced by two different conspecific birds. Song discrimination was partially maintained when the stimuli were masked by auditory distractors, but the accuracy of the discrimination progressively declined as a function of the number of masking distractors. The type of distractor used in the experiments (other conspecific songs or different types of artificial white noise) did not markedly affect the rate of deterioration of the song discrimination. These data indicate that adult male canaries have the perceptual abilities to discriminate and selectively attend to one ongoing sound that occurs simultaneously with one or more other sounds. The administration of a noradrenergic neurotoxin did not impair markedly the discrimination learning abilities although the number of subjects tested was too small to allow any firm conclusion. In these conditions, however, the noradrenergic lesion significantly increased the number failures to respond in the discrimination learning task suggesting a role, in canaries, of the noradrenergic system in some attentional processes underlying song learning and processing.},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/VARPLXU5/Appeltants et al. - 2005 - The effect of auditory distractors on song discrim.pdf;/Users/davidnicholson/Zotero/storage/G22TFSIL/S0376635705000264.html}
}

@article{aronov_specialized_2008,
  title = {A Specialized Forebrain Circuit for Vocal Babbling in the Juvenile Songbird},
  author = {Aronov, Dmitriy and Andalman, Aaron S. and Fee, Michale S.},
  year = {2008},
  journal = {Science (New York, N.Y.)},
  volume = {320},
  number = {5876},
  pages = {630--634}
}

@article{aronovSpecializedForebrainCircuit2008,
  title = {A Specialized Forebrain Circuit for Vocal Babbling in the Juvenile Songbird},
  author = {Aronov, Dmitriy and Andalman, Aaron S. and Fee, Michale S.},
  year = {2008},
  journal = {Science},
  volume = {320},
  number = {5876},
  pages = {630--634},
  file = {/Users/davidnicholson/Zotero/storage/7SUWBETM/Aronov et al. - 2008 - A specialized forebrain circuit for vocal babbling.pdf;/Users/davidnicholson/Zotero/storage/QMAEK339/630.html}
}

@misc{attrs,
  title = {Attrs},
  author = {{Hynek Schlawack}},
  year = {2020},
  month = oct
}

@misc{audacityteamAudacity2019,
  title = {Audacity},
  author = {{Audacity Team}},
  year = {2019},
  abstract = {Audacity\textregistered{} software is copyright \textcopyright{} 1999-2019 Audacity Team. Web site: https://audacityteam.org/. It is free software distributed under the terms of the GNU General Public License. The name Audacity\textregistered{} is a registered trademark of Dominic Mazzoni.}
}

@inproceedings{bendaleOpenSetDeep2016,
  title = {Towards {{Open Set Deep Networks}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Bendale, Abhijit and Boult, Terrance E.},
  year = {2016},
  month = jun,
  pages = {1563--1572},
  publisher = {{IEEE}},
  address = {{Las Vegas, NV, USA}},
  doi = {10.1109/CVPR.2016.173},
  abstract = {Deep networks have produced significant gains for various visual recognition problems, leading to high impact academic and commercial applications. Recent work in deep networks highlighted that it is easy to generate images that humans would never classify as a particular object class, yet networks classify such images high confidence as that given class \textendash{} deep network are easily fooled with images humans do not consider meaningful. The closed set nature of deep networks forces them to choose from one of the known classes leading to such artifacts. Recognition in the real world is open set, i.e. the recognition system should reject unknown/unseen classes at test time. We present a methodology to adapt deep networks for open set recognition, by introducing a new model layer, OpenMax, which estimates the probability of an input being from an unknown class. A key element of estimating the unknown probability is adapting Meta-Recognition concepts to the activation patterns in the penultimate layer of the network. OpenMax allows rejection of ``fooling'' and unrelated open set images presented to the system; OpenMax greatly reduces the number of obvious errors made by a deep network. We prove that the OpenMax concept provides bounded open space risk, thereby formally providing an open set recognition solution. We evaluate the resulting open set deep networks using pre-trained networks from the Caffe Model-zoo on ImageNet 2012 validation data, and thousands of fooling and open set images. The proposed OpenMax model significantly outperforms open set recognition accuracy of basic deep networks as well as deep networks with thresholding of SoftMax probabilities.},
  isbn = {978-1-4673-8851-1},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/FBRUBIYR/Bendale and Boult - 2016 - Towards Open Set Deep Networks.pdf}
}

@inproceedings{bendaleOpenWorldRecognition2015,
  title = {Towards {{Open World Recognition}}},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Bendale, Abhijit and Boult, Terrance},
  year = {2015},
  month = jun,
  pages = {1893--1902},
  publisher = {{IEEE}},
  address = {{Boston, MA, USA}},
  doi = {10.1109/CVPR.2015.7298799},
  abstract = {With the of advent rich classification models and high computational power visual recognition systems have found many operational applications. Recognition in the real world poses multiple challenges that are not apparent in controlled lab environments. The datasets are dynamic and novel categories must be continuously detected and then added. At prediction time, a trained system has to deal with myriad unseen categories. Operational systems require minimal downtime, even to learn. To handle these operational issues, we present the problem of Open World Recognition and formally define it. We prove that thresholding sums of monotonically decreasing functions of distances in linearly transformed feature space can balance ``open space risk'' and empirical risk. Our theory extends existing algorithms for open world recognition. We present a protocol for evaluation of open world recognition systems. We present the Nearest Non-Outlier (NNO) algorithm that evolves model efficiently, adding object categories incrementally while detecting outliers and managing open space risk. We perform experiments on the ImageNet dataset with 1.2M+ images to validate the effectiveness of our method on large scale visual recognition tasks. NNO consistently yields superior results on open world recognition.},
  isbn = {978-1-4673-6964-0},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/URYIJHRS/Bendale and Boult - 2015 - Towards Open World Recognition.pdf}
}

@article{berwick2011songs,
  ids = {berwickSongsSyntaxLinguistics2011},
  title = {Songs to Syntax: The Linguistics of Birdsong},
  author = {Berwick, Robert C and Okanoya, Kazuo and Beckers, Gabriel JL and Bolhuis, Johan J},
  year = {2011},
  journal = {Trends in cognitive sciences},
  volume = {15},
  number = {3},
  pages = {113--121},
  publisher = {{Elsevier}},
  file = {/Users/davidnicholson/Zotero/storage/VDIVUPM9/Berwick et al. - 2011 - Songs to syntax the linguistics of birdsong.pdf}
}

@inproceedings{bock_polyphonic_2012-1,
  ids = {bockPolyphonicPianoNote2012,bockPolyphonicPianoNote2012a,bock_polyphonic_2012},
  title = {Polyphonic Piano Note Transcription with Recurrent Neural Networks},
  booktitle = {2012 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {B{\"o}ck, S. and Schedl, M.},
  year = {2012},
  month = mar,
  pages = {121--124},
  doi = {10.1109/ICASSP.2012.6287832},
  abstract = {In this paper a new approach for polyphonic piano note onset transcription is presented. It is based on a recurrent neural network to simultaneously detect the onsets and the pitches of the notes from spectral features. Long Short-Term Memory units are used in a bidirectional neural network to model the context of the notes. The use of a single regression output layer instead of the often used one-versus-all classification approach enables the system to significantly lower the number of erroneous note detections. Evaluation is based on common test sets and shows exceptional temporal precision combined with a significant boost in note transcription performance compared to current state-of-the-art approaches. The system is trained jointly with various synthesized piano instruments and real piano recordings and thus generalizes much better than existing systems.},
  keywords = {Accuracy,bidirectional neural network,common test sets,erroneous note detections,exceptional temporal precision,Hidden Markov models,information retrieval,Instruments,long short-term memory units,music,music information retrieval,musical instruments,neural networks,note transcription performance,one-versus-all classification approach,piano recordings,polyphonic piano note onset transcription,polyphonic piano note transcription,recurrent neural nets,recurrent neural networks,Recurrent neural networks,regression analysis,signal classification,single regression output layer,spectral analysis,spectral features,Spectrogram,synthesized piano instruments,Training},
  file = {/Users/davidnicholson/Zotero/storage/2433GN2E/Böck and Schedl - 2012 - Polyphonic piano note transcription with recurrent.pdf;/Users/davidnicholson/Zotero/storage/LTMFT85M/Böck and Schedl - 2012 - Polyphonic piano note transcription with recurrent.pdf;/Users/davidnicholson/Zotero/storage/DXG3WYSJ/6287832.html;/Users/davidnicholson/Zotero/storage/RRZ2G9XR/6287832.html}
}

@inproceedings{bock_polyphonic_2012-1,
  title = {Polyphonic Piano Note Transcription with Recurrent Neural Networks},
  booktitle = {2012 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {B{\"o}ck, S. and Schedl, M.},
  year = {2012},
  month = mar,
  pages = {121--124},
  doi = {10.1109/ICASSP.2012.6287832},
  abstract = {In this paper a new approach for polyphonic piano note onset transcription is presented. It is based on a recurrent neural network to simultaneously detect the onsets and the pitches of the notes from spectral features. Long Short-Term Memory units are used in a bidirectional neural network to model the context of the notes. The use of a single regression output layer instead of the often used one-versus-all classification approach enables the system to significantly lower the number of erroneous note detections. Evaluation is based on common test sets and shows exceptional temporal precision combined with a significant boost in note transcription performance compared to current state-of-the-art approaches. The system is trained jointly with various synthesized piano instruments and real piano recordings and thus generalizes much better than existing systems.},
  keywords = {Accuracy,bidirectional neural network,common test sets,erroneous note detections,exceptional temporal precision,Hidden Markov models,information retrieval,Instruments,long short-term memory units,music,music information retrieval,musical instruments,neural networks,note transcription performance,one-versus-all classification approach,piano recordings,polyphonic piano note onset transcription,polyphonic piano note transcription,recurrent neural nets,recurrent neural networks,Recurrent neural networks,regression analysis,signal classification,single regression output layer,spectral analysis,spectral features,Spectrogram,synthesized piano instruments,Training}
}

@article{brainard_what_2002,
  title = {What Songbirds Teach Us about Learning},
  author = {Brainard, Michael S. and Doupe, Allison J.},
  year = {2002},
  journal = {Nature},
  volume = {417},
  number = {6886},
  pages = {351--358}
}

@article{brainardWhatSongbirdsTeach2002,
  title = {What Songbirds Teach Us about Learning},
  author = {Brainard, Michael S. and Doupe, Allison J.},
  year = {2002},
  journal = {Nature},
  volume = {417},
  number = {6886},
  pages = {351--358},
  file = {/Users/davidnicholson/Zotero/storage/A6YWX6TD/Brainard and Doupe - 2002 - What songbirds teach us about learning.pdf;/Users/davidnicholson/Zotero/storage/PESBYWFS/417351a.html}
}

@article{burkett2015voice,
  title = {{{VoICE}}: {{A}} Semi-Automated Pipeline for Standardizing Vocal Analysis across Models},
  author = {Burkett, Zachary D and Day, Nancy F and Pe{\~n}agarikano, Olga and Geschwind, Daniel H and White, Stephanie A},
  year = {2015},
  journal = {Scientific reports},
  volume = {5},
  pages = {10237},
  publisher = {{Nature Publishing Group}}
}

@misc{casper_da_costa_luis_2020_4054194,
  title = {Tqdm: {{A}} Fast, Extensible Progress Bar for Python and {{CLI}}},
  author = {{da Costa-Luis}, Casper and Larroque, Stephen Karl and Altendorf, Kyle and Mary, Hadrien and Korobov, Mikhail and {Yorav-Raphael}, Noam and Ivanov, Ivan and Bargull, Marcel and Rodrigues, Nishant and CHEN, Guangshuo and Newey, Charles and {James} and Zugnoni, Martin and Pagel, Matthew D. and {mjstevens777} and Dektyarev, Mikhail and Rothberg, Alex and {Alexander} and Panteleit, Daniel and Dill, Fabian and {FichteFoll} and {HeoHeo} and {van Kemenade}, Hugo and McCracken, Jack and Nordlund, Max and Desh, Orivej and {RedBug312} and {richardsheridan} and {Socialery} and Malmgren, Staffan},
  year = {2020},
  month = sep,
  doi = {10.5281/zenodo.4054194},
  howpublished = {Zenodo}
}

@inproceedings{Chen:2016:XST:2939672.2939785,
  title = {{{XGBoost}}: {{A}} Scalable Tree Boosting System},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD}} International Conference on Knowledge Discovery and Data Mining},
  author = {Chen, Tianqi and Guestrin, Carlos},
  year = {2016},
  series = {{{KDD}} '16},
  pages = {785--794},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2939672.2939785},
  acmid = {2939785},
  isbn = {978-1-4503-4232-2},
  keywords = {large-scale machine learning}
}

@misc{chenICGY96AwesomeOpenSetRecognition2021,
  title = {{{iCGY96}}/awesome\_{{OpenSetRecognition}}\_list},
  author = {Chen, Guangyao},
  year = {2021},
  month = jul,
  abstract = {A curated list of papers \& resources linked to open set recognition, out-of-distribution, open set domain adaptation and open world recognition},
  keywords = {awesome-list,open-set,open-set-domain-adaptation,open-set-recognition,open-source,open-word-recognition,out-of-distribution-detection}
}

@article{coffey_deepsqueak_2019,
  title = {{{DeepSqueak}}: A Deep Learning-Based System for Detection and Analysis of Ultrasonic Vocalizations},
  shorttitle = {{{DeepSqueak}}},
  author = {Coffey, Kevin R. and Marx, Russell G. and Neumaier, John F.},
  year = {2019},
  month = apr,
  journal = {Neuropsychopharmacology : official publication of the American College of Neuropsychopharmacology},
  volume = {44},
  number = {5},
  pages = {859--868},
  issn = {1740-634X},
  doi = {10.1038/s41386-018-0303-6},
  abstract = {Rodents engage in social communication through a rich repertoire of ultrasonic vocalizations (USVs). Recording and analysis of USVs has broad utility during diverse behavioral tests and can be performed noninvasively in almost any rodent behavioral model to provide rich insights into the emotional state and motor function of the test animal. Despite strong evidence that USVs serve an array of communicative functions, technical and financial limitations have been barriers for most laboratories to adopt vocalization analysis. Recently, deep learning has revolutionized the field of machine hearing and vision, by allowing computers to perform human-like activities including seeing, listening, and speaking. Such systems are constructed from biomimetic, ``deep'', artificial neural networks. Here, we present DeepSqueak, a USV detection and analysis software suite that can perform human quality USV detection and classification automatically, rapidly, and reliably using cutting-edge regional convolutional neural network architecture (Faster-RCNN). DeepSqueak was engineered to allow non-experts easy entry into USV detection and analysis yet is flexible and adaptable with a graphical user interface and offers access to numerous input and analysis features. Compared to other modern programs and manual analysis, DeepSqueak was able to reduce false positives, increase detection recall, dramatically reduce analysis time, optimize automatic syllable classification, and perform automatic syntax analysis on arbitrarily large numbers of syllables, all while maintaining manual selection review and supervised classification. DeepSqueak allows USV recording and analysis to be added easily to existing rodent behavioral procedures, hopefully revealing a wide range of innate responses to provide another dimension of insights into behavior when combined with conventional outcome measures.},
  copyright = {2019 American College of Neuropsychopharmacology},
  langid = {english}
}

@article{coffeyDeepSqueakDeepLearningbased2019,
  title = {{{DeepSqueak}}: A Deep Learning-Based System for Detection and Analysis of Ultrasonic Vocalizations},
  shorttitle = {{{DeepSqueak}}},
  author = {Coffey, Kevin R. and Marx, Russell G. and Neumaier, John F.},
  year = {2019},
  month = apr,
  journal = {Neuropsychopharmacology},
  volume = {44},
  number = {5},
  pages = {859--868},
  publisher = {{Nature Publishing Group}},
  issn = {1740-634X},
  doi = {10.1038/s41386-018-0303-6},
  abstract = {Rodents engage in social communication through a rich repertoire of ultrasonic vocalizations (USVs). Recording and analysis of USVs has broad utility during diverse behavioral tests and can be performed noninvasively in almost any rodent behavioral model to provide rich insights into the emotional state and motor function of the test animal. Despite strong evidence that USVs serve an array of communicative functions, technical and financial limitations have been barriers for most laboratories to adopt vocalization analysis. Recently, deep learning has revolutionized the field of machine hearing and vision, by allowing computers to perform human-like activities including seeing, listening, and speaking. Such systems are constructed from biomimetic, ``deep'', artificial neural networks. Here, we present DeepSqueak, a USV detection and analysis software suite that can perform human quality USV detection and classification automatically, rapidly, and reliably using cutting-edge regional convolutional neural network architecture (Faster-RCNN). DeepSqueak was engineered to allow non-experts easy entry into USV detection and analysis yet is flexible and adaptable with a graphical user interface and offers access to numerous input and analysis features. Compared to other modern programs and manual analysis, DeepSqueak was able to reduce false positives, increase detection recall, dramatically reduce analysis time, optimize automatic syllable classification, and perform automatic syntax analysis on arbitrarily large numbers of syllables, all while maintaining manual selection review and supervised classification. DeepSqueak allows USV recording and analysis to be added easily to existing rodent behavioral procedures, hopefully revealing a wide range of innate responses to provide another dimension of insights into behavior when combined with conventional outcome measures.},
  copyright = {2019 American College of Neuropsychopharmacology},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/77XSKW37/Coffey et al. - 2019 - DeepSqueak a deep learning-based system for detec.pdf;/Users/davidnicholson/Zotero/storage/QX87FGKH/s41386-018-0303-6.html}
}

@article{coffeyDeepSqueakDeepLearningbased2019a,
  title = {{{DeepSqueak}}: A Deep Learning-Based System for Detection and Analysis of Ultrasonic Vocalizations},
  author = {Coffey, Kevin R},
  year = {2019},
  pages = {10},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/T3EV63PT/Coffey - 2019 - DeepSqueak a deep learning-based system for detec.pdf}
}

@article{cohen_hidden_2020,
  title = {Hidden Neural States Underlie Canary Song Syntax},
  author = {Cohen, Yarden and Shen, Jun and Semu, Dawit and Leman, Daniel P. and Liberti, William A. and Perkins, L. Nathan and Liberti, Derek C. and Kotton, Darrell N. and Gardner, Timothy J.},
  year = {2020},
  month = jun,
  journal = {Nature},
  volume = {582},
  number = {7813},
  pages = {539--544},
  issn = {1476-4687},
  doi = {10.1038/s41586-020-2397-3},
  abstract = {Coordinated skills such as speech or dance involve sequences of actions that follow syntactic rules in which transitions between elements depend on the identities and order of past actions. Canary songs consist of repeated syllables called phrases, and the ordering of these phrases follows long-range rules1 in which the choice of what to sing depends on the song structure many seconds prior. The neural substrates that support these long-range correlations are unknown. Here, using miniature head-mounted microscopes and cell-type-specific genetic tools, we observed neural activity in the premotor nucleus HVC2\textendash 4 as canaries explored various phrase sequences in their repertoire. We identified neurons that encode past transitions, extending over four phrases and spanning up to four seconds and forty syllables. These neurons preferentially encode past actions rather than future actions, can reflect more than one song history, and are active mostly during the rare phrases that involve history-dependent transitions in song. These findings demonstrate that the dynamics of HVC include `hidden states' that are not reflected in ongoing behaviour but rather carry information about prior actions. These states provide a possible substrate for the control of syntax transitions governed by long-range rules.},
  copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english}
}

@article{cohenHiddenNeuralStates2020,
  title = {Hidden Neural States Underlie Canary Song Syntax},
  author = {Cohen, Yarden and Shen, Jun and Semu, Dawit and Leman, Daniel P. and Liberti, William A. and Perkins, L. Nathan and Liberti, Derek C. and Kotton, Darrell N. and Gardner, Timothy J.},
  year = {2020},
  month = jun,
  journal = {Nature},
  volume = {582},
  number = {7813},
  pages = {539--544},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-020-2397-3},
  abstract = {Coordinated skills such as speech or dance involve sequences of actions that follow syntactic rules in which transitions between elements depend on the identities and order of past actions. Canary songs consist of repeated syllables called phrases, and the ordering of these phrases follows long-range rules1 in which the choice of what to sing depends on the song structure many seconds prior. The neural substrates that support these long-range correlations are unknown. Here, using miniature head-mounted microscopes and cell-type-specific genetic tools, we observed neural activity in the premotor nucleus HVC2\textendash 4 as canaries explored various phrase sequences in their repertoire. We identified neurons that encode past transitions, extending over four phrases and spanning up to four seconds and forty syllables. These neurons preferentially encode past actions rather than future actions, can reflect more than one song history, and are active mostly during the rare phrases that involve history-dependent transitions in song. These findings demonstrate that the dynamics of HVC include `hidden states' that are not reflected in ongoing behaviour but rather carry information about prior actions. These states provide a possible substrate for the control of syntax transitions governed by long-range rules.},
  copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/R69ASI6P/Cohen et al. - 2020 - Hidden neural states underlie canary song syntax.pdf;/Users/davidnicholson/Zotero/storage/7L2WRFFT/s41586-020-2397-3.html}
}

@article{daou2012computational,
  title = {A Computational Tool for Automated Large-Scale Analysis and Measurement of Bird-Song Syntax},
  author = {Daou, Arij and Johnson, Frank and Wu, Wei and Bertram, Richard},
  year = {2012},
  journal = {Journal of neuroscience methods},
  volume = {210},
  number = {2},
  pages = {147--160},
  publisher = {{Elsevier}}
}

@book{dask_development_team_dask_2016,
  title = {Dask: {{Library}} for Dynamic Task Scheduling},
  author = {{Dask Development Team}},
  year = {2016}
}

@book{daskdevelopmentteamDaskLibraryDynamic2016a,
  title = {Dask: {{Library}} for Dynamic Task Scheduling},
  author = {{Dask Development Team}},
  year = {2016}
}

@misc{david_nicholson_2021_4584209,
  title = {{{NickleDave}}/Evfuncs:},
  author = {Nicholson, David},
  year = {2021},
  month = mar,
  doi = {10.5281/zenodo.4584209},
  howpublished = {Zenodo}
}

@misc{david_nicholson_2021_4584210,
  title = {{{NickleDave}}/Koumura:},
  author = {Nicholson, David},
  year = {2021},
  month = mar,
  doi = {10.5281/zenodo.4584210},
  howpublished = {Zenodo}
}

@article{david_nicholson_yardencsgithub/tweetynet_2019,
  title = {{{yardencsGitHub}}/Tweetynet 0.2.0},
  author = {Nicholson, David and {yardencsGitHub}},
  year = {2019},
  month = may,
  publisher = {{Zenodo}},
  doi = {10.5281/zenodo.2667818},
  abstract = {Added add gardner package with yarden2seq module that converts annotation.mat files to crowsetta.Sequence objects will be installed along with TweetyNet so that vak knows how to use yarden format}
}

@misc{david_nicholson_yardencsgithub/tweetynet_2019,
  title = {{{yardencsGitHub}}/Tweetynet 0.2.0},
  author = {Nicholson, David and {yardencsGitHub}},
  year = {2019},
  month = may,
  publisher = {{Zenodo}},
  doi = {10.5281/zenodo.2667818},
  abstract = {Added add gardner package with yarden2seq module that converts annotation.mat files to crowsetta.Sequence objects will be installed along with TweetyNet so that vak knows how to use yarden format}
}

@article{david_nicholson_yardencsgithub/tweetynet_2019,
  title = {{{yardencsGitHub}}/Tweetynet 0.2.0},
  author = {Nicholson, David and {yardencsGitHub}},
  year = {2019},
  month = may,
  doi = {10.5281/zenodo.2667818},
  abstract = {Added add gardner package with yarden2seq module that converts annotation.mat files to crowsetta.Sequence objects will be installed along with TweetyNet so that vak knows how to use yarden format}
}

@misc{davidnicholsonYardencsGitHubTweetynet2019,
  title = {{{yardencsGitHub}}/Tweetynet 0.2.0},
  author = {David Nicholson and {yardencsGitHub}},
  year = {2019},
  month = may,
  doi = {10.5281/zenodo.2667818},
  abstract = {Added add gardner package with yarden2seq module that converts annotation.mat files to crowsetta.Sequence objects will be installed along with TweetyNet so that vak knows how to use yarden format},
  howpublished = {Zenodo},
  file = {/Users/davidnicholson/Zotero/storage/5G56L3R2/2667818.html}
}

@article{deregnaucourt_how_2005,
  title = {How Sleep Affects the Developmental Learning of Bird Song},
  author = {Der{\'e}gnaucourt, S{\'e}bastien and Mitra, Partha P. and Feh{\'e}r, Olga and Pytte, Carolyn and Tchernichovski, Ofer},
  year = {2005},
  journal = {Nature},
  volume = {433},
  number = {7027},
  pages = {710--716}
}

@article{deregnaucourtHowSleepAffects2005,
  title = {How Sleep Affects the Developmental Learning of Bird Song},
  author = {Der{\'e}gnaucourt, S{\'e}bastien and Mitra, Partha P. and Feh{\'e}r, Olga and Pytte, Carolyn and Tchernichovski, Ofer},
  year = {2005},
  journal = {Nature},
  volume = {433},
  number = {7027},
  pages = {710--716},
  file = {/Users/davidnicholson/Zotero/storage/B8A4TC8D/Derégnaucourt et al. - 2005 - How sleep affects the developmental learning of bi.pdf;/Users/davidnicholson/Zotero/storage/6JIYCB6X/nature03275.html}
}

@article{dhamijaReducingNetworkAgnostophobia2018,
  title = {Reducing {{Network Agnostophobia}}},
  author = {Dhamija, Akshay Raj and G{\"u}nther, Manuel and Boult, Terrance E.},
  year = {2018},
  month = dec,
  journal = {arXiv:1811.04110 [cs]},
  eprint = {1811.04110},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Agnostophobia, the fear of the unknown, can be experienced by deep learning engineers while applying their networks to real-world applications. Unfortunately, network behavior is not well defined for inputs far from a networks training set. In an uncontrolled environment, networks face many instances that are not of interest to them and have to be rejected in order to avoid a false positive. This problem has previously been tackled by researchers by either a) thresholding softmax, which by construction cannot return "none of the known classes", or b) using an additional background or garbage class. In this paper, we show that both of these approaches help, but are generally insufficient when previously unseen classes are encountered. We also introduce a new evaluation metric that focuses on comparing the performance of multiple approaches in scenarios where such unseen classes or unknowns are encountered. Our major contributions are simple yet effective Entropic Open-Set and Objectosphere losses that train networks using negative samples from some classes. These novel losses are designed to maximize entropy for unknown inputs while increasing separation in deep feature space by modifying magnitudes of known and unknown samples. Experiments on networks trained to classify classes from MNIST and CIFAR-10 show that our novel loss functions are significantly better at dealing with unknown inputs from datasets such as Devanagari, NotMNIST, CIFAR-100, and SVHN.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/davidnicholson/Zotero/storage/LN5MHSFU/Dhamija et al. - 2018 - Reducing Network Agnostophobia.pdf;/Users/davidnicholson/Zotero/storage/G4FGUKE3/1811.html}
}

@article{farabet_learning_2013,
  title = {Learning {{Hierarchical Features}} for {{Scene Labeling}}},
  author = {Farabet, Clement and Couprie, Camille and Najman, Laurent and LeCun, Yann},
  year = {2013},
  month = aug,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {35},
  number = {8},
  pages = {1915--1929},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2012.231},
  abstract = {Scene labeling consists of labeling each pixel in an image with the category of the object it belongs to. We propose a method that uses a multiscale convolutional network trained from raw pixels to extract dense feature vectors that encode regions of multiple sizes centered on each pixel. The method alleviates the need for engineered features, and produces a powerful representation that captures texture, shape, and contextual information. We report results using multiple postprocessing methods to produce the final labeling. Among those, we propose a technique to automatically retrieve, from a pool of segmentation components, an optimal set of components that best explain the scene; these components are arbitrary, for example, they can be taken from a segmentation tree or from any family of oversegmentations. The system yields record accuracies on the SIFT Flow dataset (33 classes) and the Barcelona dataset (170 classes) and near-record accuracy on Stanford background dataset (eight classes), while being an order of magnitude faster than competing approaches, producing a 320\texttimes 240 image labeling in less than a second, including feature extraction.}
}

@article{farabetLearningHierarchicalFeatures2013,
  title = {Learning {{Hierarchical Features}} for {{Scene Labeling}}},
  author = {Farabet, Clement and Couprie, Camille and Najman, Laurent and LeCun, Yann},
  year = {2013},
  month = aug,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {35},
  number = {8},
  pages = {1915--1929},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2012.231},
  abstract = {Scene labeling consists of labeling each pixel in an image with the category of the object it belongs to. We propose a method that uses a multiscale convolutional network trained from raw pixels to extract dense feature vectors that encode regions of multiple sizes centered on each pixel. The method alleviates the need for engineered features, and produces a powerful representation that captures texture, shape, and contextual information. We report results using multiple postprocessing methods to produce the final labeling. Among those, we propose a technique to automatically retrieve, from a pool of segmentation components, an optimal set of components that best explain the scene; these components are arbitrary, for example, they can be taken from a segmentation tree or from any family of oversegmentations. The system yields record accuracies on the SIFT Flow dataset (33 classes) and the Barcelona dataset (170 classes) and near-record accuracy on Stanford background dataset (eight classes), while being an order of magnitude faster than competing approaches, producing a 320\texttimes 240 image labeling in less than a second, including feature extraction.},
  file = {/Users/davidnicholson/Zotero/storage/QTG34R32/Farabet et al. - 2013 - Learning Hierarchical Features for Scene Labeling.pdf;/Users/davidnicholson/Zotero/storage/KIVTIUBW/6338939.html}
}

@article{fee_songbird_2010,
  title = {The Songbird as a Model for the Generation and Learning of Complex Sequential Behaviors},
  author = {Fee, Michale S. and Scharff, Constance},
  year = {2010},
  journal = {ILAR journal},
  volume = {51},
  number = {4},
  pages = {362--377}
}

@article{feeSongbirdModelGeneration2010,
  title = {The Songbird as a Model for the Generation and Learning of Complex Sequential Behaviors},
  author = {Fee, Michale S. and Scharff, Constance},
  year = {2010},
  journal = {ILAR journal},
  volume = {51},
  number = {4},
  pages = {362--377},
  file = {/Users/davidnicholson/Zotero/storage/6NL2X4Y7/Fee and Scharff - 2010 - The songbird as a model for the generation and lea.pdf;/Users/davidnicholson/Zotero/storage/QJ8UQU9L/676057.html}
}

@article{fonseca2021analysis,
  title = {Analysis of Ultrasonic Vocalizations from Mice Using Computer Vision and Machine Learning},
  author = {Fonseca, Antonio HO and Santana, Gustavo M and Ortiz, Gabriela M Bosque and Bampi, S{\'e}rgio and Dietrich, Marcelo O},
  year = {2021},
  journal = {Elife},
  volume = {10},
  pages = {e59161},
  publisher = {{eLife Sciences Publications Limited}}
}

@article{gardner_freedom_2005,
  ids = {gardnerFreedomRulesAcquisition2005},
  title = {Freedom and Rules: The Acquisition and Reprogramming of a Bird's Learned Song},
  shorttitle = {Freedom and Rules},
  author = {Gardner, Timothy J. and Naef, Felix and Nottebohm, Fernando},
  year = {2005},
  month = may,
  journal = {Science (New York, N.Y.)},
  volume = {308},
  number = {5724},
  pages = {1046--1049},
  issn = {1095-9203},
  doi = {10.1126/science.1108214},
  abstract = {Canary song is hierarchically structured: Short stereotyped syllables are repeated to form phrases, which in turn are arranged to form songs. This structure occurs even in the songs of young isolates, which suggests that innate rules govern canary song development. However, juveniles that had never heard normal song imitated abnormal synthetic songs with great accuracy, even when the tutor songs lacked phrasing. As the birds matured, imitated songs were reprogrammed to form typical canary phrasing. Thus, imitation and innate song constraints are separate processes that can be segregated in time: freedom in youth, rules in adulthood.},
  langid = {english},
  pmid = {15890887},
  keywords = {Aging,Animal,Animals,Canaries,Female,Imitative Behavior,Learning,Male,Memory,Sexual Maturation,Testosterone,Vocalization,Vocalization; Animal}
}

@article{gardner_freedom_2005,
  title = {Freedom and Rules: The Acquisition and Reprogramming of a Bird's Learned Song},
  shorttitle = {Freedom and Rules},
  author = {Gardner, Timothy J. and Naef, Felix and Nottebohm, Fernando},
  year = {2005},
  month = may,
  journal = {Science (New York, N.Y.)},
  volume = {308},
  number = {5724},
  pages = {1046--1049},
  issn = {1095-9203},
  doi = {10.1126/science.1108214},
  abstract = {Canary song is hierarchically structured: Short stereotyped syllables are repeated to form phrases, which in turn are arranged to form songs. This structure occurs even in the songs of young isolates, which suggests that innate rules govern canary song development. However, juveniles that had never heard normal song imitated abnormal synthetic songs with great accuracy, even when the tutor songs lacked phrasing. As the birds matured, imitated songs were reprogrammed to form typical canary phrasing. Thus, imitation and innate song constraints are separate processes that can be segregated in time: freedom in youth, rules in adulthood.},
  langid = {english},
  pmid = {15890887},
  keywords = {Aging,Animal,Animals,Canaries,Female,Imitative Behavior,Learning,Male,Memory,Sexual Maturation,Testosterone,Vocalization}
}

@article{gengRecentAdvancesOpen2020,
  title = {Recent {{Advances}} in {{Open Set Recognition}}: {{A Survey}}},
  shorttitle = {Recent {{Advances}} in {{Open Set Recognition}}},
  author = {Geng, Chuanxing and Huang, Sheng-jun and Chen, Songcan},
  year = {2020},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  eprint = {1811.08581},
  eprinttype = {arxiv},
  pages = {1--1},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2020.2981604},
  abstract = {In real-world recognition/classification tasks, limited by various objective factors, it is usually difficult to collect training samples to exhaust all classes when training a recognizer or classifier. A more realistic scenario is open set recognition (OSR), where incomplete knowledge of the world exists at training time, and unknown classes can be submitted to an algorithm during testing, requiring the classifiers to not only accurately classify the seen classes, but also effectively deal with unseen ones. This paper provides a comprehensive survey of existing open set recognition techniques covering various aspects ranging from related definitions, representations of models, datasets, evaluation criteria, and algorithm comparisons. Furthermore, we briefly analyze the relationships between OSR and its related tasks including zero-shot, one-shot (few-shot) recognition/learning techniques, classification with reject option, and so forth. Additionally, we also review the open world recognition which can be seen as a natural extension of OSR. Importantly, we highlight the limitations of existing approaches and point out some promising subsequent research directions in this field.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/davidnicholson/Zotero/storage/BFB5C8VE/Geng et al. - 2020 - Recent Advances in Open Set Recognition A Survey.pdf}
}

@article{goffinet_inferring_2019,
  title = {Inferring Low-Dimensional Latent Descriptions of Animal Vocalizations},
  author = {Goffinet, Jack and Mooney, Richard and Pearson, John},
  year = {2019},
  month = oct,
  journal = {bioRxiv : the preprint server for biology},
  pages = {811661},
  doi = {10.1101/811661},
  abstract = {{$<$}h3{$>$}ABSTRACT{$<$}/h3{$>$} {$<$}p{$>$}Vocalization is an essential medium for social and sexual signaling in most birds and mammals. Consequently, the analysis of vocal behavior is of great interest to fields such as neuroscience and linguistics. A standard approach to analyzing vocalization involves segmenting the sound stream into discrete vocal elements, calculating a number of handpicked acoustic features, and then using the feature values for subsequent quantitative analysis. While this approach has proven powerful, it suffers from several crucial limitations: First, handpicked acoustic features may miss important dimensions of variability that are important for communicative function. Second, many analyses assume vocalizations fall into discrete vocal categories, often without rigorous justification. Third, a syllable-level analysis requires a consistent definition of syllable boundaries, which is often difficult to maintain in practice and limits the sorts of structure one can find in the data. To address these shortcomings, we apply a data-driven approach based on the variational autoencoder (VAE), an unsupervised learning method, to the task of characterizing vocalizations in two model species: the laboratory mouse (\emph{Mus musculus}) and the zebra finch (\emph{Taeniopygia guttata}). We find that the VAE converges on a parsimonious representation of vocal behavior that outperforms handpicked acoustic features on a variety of common analysis tasks, including representing acoustic similarity and recovering a known effect of social context on birdsong. Additionally, we use our learned acoustic features to argue against the widespread view that mouse ultrasonic vocalizations form discrete syllable categories. Lastly, we present a novel ``shotgun VAE'' that can quantify moment-by-moment variability in vocalizations. In all, we show that data-derived acoustic features confirm and extend existing approaches while offering distinct advantages in several critical applications.{$<$}/p{$>$}},
  copyright = {\textcopyright{} 2019, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
  langid = {english}
}

@article{goffinetInferringLowdimensionalLatent2019,
  title = {Inferring Low-Dimensional Latent Descriptions of Animal Vocalizations},
  author = {Goffinet, Jack and Mooney, Richard and Pearson, John},
  year = {2019},
  month = oct,
  journal = {bioRxiv},
  pages = {811661},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/811661},
  abstract = {{$<$}h3{$>$}ABSTRACT{$<$}/h3{$>$} {$<$}p{$>$}Vocalization is an essential medium for social and sexual signaling in most birds and mammals. Consequently, the analysis of vocal behavior is of great interest to fields such as neuroscience and linguistics. A standard approach to analyzing vocalization involves segmenting the sound stream into discrete vocal elements, calculating a number of handpicked acoustic features, and then using the feature values for subsequent quantitative analysis. While this approach has proven powerful, it suffers from several crucial limitations: First, handpicked acoustic features may miss important dimensions of variability that are important for communicative function. Second, many analyses assume vocalizations fall into discrete vocal categories, often without rigorous justification. Third, a syllable-level analysis requires a consistent definition of syllable boundaries, which is often difficult to maintain in practice and limits the sorts of structure one can find in the data. To address these shortcomings, we apply a data-driven approach based on the variational autoencoder (VAE), an unsupervised learning method, to the task of characterizing vocalizations in two model species: the laboratory mouse (\emph{Mus musculus}) and the zebra finch (\emph{Taeniopygia guttata}). We find that the VAE converges on a parsimonious representation of vocal behavior that outperforms handpicked acoustic features on a variety of common analysis tasks, including representing acoustic similarity and recovering a known effect of social context on birdsong. Additionally, we use our learned acoustic features to argue against the widespread view that mouse ultrasonic vocalizations form discrete syllable categories. Lastly, we present a novel ``shotgun VAE'' that can quantify moment-by-moment variability in vocalizations. In all, we show that data-derived acoustic features confirm and extend existing approaches while offering distinct advantages in several critical applications.{$<$}/p{$>$}},
  chapter = {New Results},
  copyright = {\textcopyright{} 2019, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/INPEQWXT/Goffinet et al. - 2019 - Inferring low-dimensional latent descriptions of a.pdf;/Users/davidnicholson/Zotero/storage/W4S4JZY4/811661v1.html}
}

@article{goffinetLowdimensionalLearnedFeature2021,
  title = {Low-Dimensional Learned Feature Spaces Quantify Individual and Group Differences in Vocal Repertoires},
  author = {Goffinet, Jack and Brudner, Samuel and Mooney, Richard and Pearson, John},
  year = {2021},
  month = may,
  journal = {eLife},
  volume = {10},
  pages = {e67855},
  issn = {2050-084X},
  doi = {10.7554/eLife.67855},
  abstract = {Increases in the scale and complexity of behavioral data pose an increasing challenge for data analysis. A common strategy involves replacing entire behaviors with small numbers of handpicked, domain-\-specific features, but this approach suffers from several crucial limitations. For example, handpicked features may miss important dimensions of variability, and correlations among them complicate statistical testing. Here, by contrast, we apply the variational autoencoder (VAE), an unsupervised learning method, to learn features directly from data and quantify the vocal behavior of two model species: the laboratory mouse and the zebra finch. The VAE converges on a parsimonious representation that outperforms handpicked features on a variety of common analysis tasks, enables the measurement of moment-\-by-m\- oment vocal variability on the timescale of tens of milliseconds in the zebra finch, provides strong evidence that mouse ultrasonic vocalizations do not cluster as is commonly believed, and captures the similarity of tutor and pupil birdsong with qualitatively higher fidelity than previous approaches. In all, we demonstrate the utility of modern unsupervised learning approaches to the quantification of complex and high-\-dimensional vocal behavior.},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/DIEWKDBT/Goffinet et al. - 2021 - Low-dimensional learned feature spaces quantify in.pdf}
}

@article{goldmanNeuronalProductionMigration1983,
  title = {Neuronal Production, Migration, and Differentiation in a Vocal Control Nucleus of the Adult Female Canary Brain},
  author = {Goldman, S. A. and Nottebohm, F.},
  year = {1983},
  month = apr,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {80},
  number = {8},
  pages = {2390--2394},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.80.8.2390},
  abstract = {The vocal control nucleus designated HVc (hyperstriatum ventrale, pars caudalis) of adult female canaries expands in response to systemic testosterone administration, which also induces the females to sing in a male-like manner. We became interested in the possibility of neurogenesis as a potential basis for this phenomenon. Intact adult female canaries were injected with [3H]thymidine over a 2-day period. Some birds were given testosterone implants at various times before thymidine. The birds were sacrificed 5 wk after hormone implantation, and their brains were processed for autoradiography. In parallel control experiments, some birds were given implants of cholesterol instead of testosterone. All birds showed considerable numbers of labeled neurons, glia, endothelia, and ventricular zone cells in and around HVc. Ultrastructural analysis confirmed the identity of these labeled neurons. Cholesterol- and testosterone-treated birds had similar neuronal labeling indices, which ranged from 1.8\% to 4.0\% in HVc. Thus, neurogenesis occurred in these adults independently of exogenous hormone treatment. Conversely, both glial and endothelial proliferation rates were markedly stimulated by exogenous testosterone treatment. We determined the origin of the thymidine-incorporating neurons by sacrificing two thymidine-treated females soon after their thymidine injections, precluding any significant migration of newly labeled cells. Analysis of these brains revealed no cells of neuronal morphology present in HVc but a very heavily labeled ventricular zone overlying HVc. We conclude that neuronal precursors exist in the HVc ventricular zone that incorporate tritiated thymidine during the S phase preceding their mitosis; after division these cells migrate into, and to some extent beyond, HVc. This ventricular zone neurogenesis seems to be a normally occurring phenomenon in intact adult female canaries.},
  chapter = {Research Article},
  langid = {english},
  pmid = {6572982},
  file = {/Users/davidnicholson/Zotero/storage/UHS3PPEL/Goldman and Nottebohm - 1983 - Neuronal production, migration, and differentiatio.pdf;/Users/davidnicholson/Zotero/storage/78XRJ8X7/2390.html}
}

@book{goodfellow_deep_2016,
  title = {Deep Learning},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  publisher = {{MIT press}}
}

@book{goodfellow_deep_2016-1,
  title = {Deep {{Learning}}},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  month = nov,
  publisher = {{MIT Press}},
  abstract = {An introduction to a broad range of topics in deep learning, covering mathematical and conceptual background, deep learning techniques used in industry, and research perspectives.``Written by three experts in the field, Deep Learning is the only comprehensive book on the subject.''\textemdash Elon Musk, cochair of OpenAI; cofounder and CEO of Tesla and SpaceXDeep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors.},
  isbn = {978-0-262-03561-3},
  langid = {english}
}

@book{goodfellowDeepLearning2016,
  title = {Deep Learning},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  publisher = {{MIT press}},
  file = {/Users/davidnicholson/Zotero/storage/7T2U3JUV/Goodfellow et al. - 2016 - Deep learning.pdf;/Users/davidnicholson/Zotero/storage/VR2X9MV8/books.html}
}

@book{goodfellowDeepLearning2016a,
  title = {Deep {{Learning}}},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  month = nov,
  publisher = {{MIT Press}},
  abstract = {An introduction to a broad range of topics in deep learning, covering mathematical and conceptual background, deep learning techniques used in industry, and research perspectives.``Written by three experts in the field, Deep Learning is the only comprehensive book on the subject.''\textemdash Elon Musk, cochair of OpenAI; cofounder and CEO of Tesla and SpaceXDeep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors.},
  googlebooks = {Np9SDQAAQBAJ},
  isbn = {978-0-262-03561-3},
  langid = {english}
}

@article{graves_framewise_2005,
  ids = {gravesFramewisePhonemeClassification2005},
  title = {Framewise Phoneme Classification with Bidirectional {{LSTM}} and Other Neural Network Architectures},
  author = {Graves, Alex and Schmidhuber, J{\"u}rgen},
  year = {2005},
  journal = {Neural networks},
  volume = {18},
  number = {5-6},
  pages = {602--610},
  file = {/Users/davidnicholson/Zotero/storage/6494VG59/Graves and Schmidhuber - 2005 - Framewise phoneme classification with bidirectiona.pdf;/Users/davidnicholson/Zotero/storage/ZHDXWXGM/S0893608005001206.html}
}

@article{graves_framewise_2005,
  title = {Framewise Phoneme Classification with Bidirectional {{LSTM}} and Other Neural Network Architectures},
  author = {Graves, Alex and Schmidhuber, J{\"u}rgen},
  year = {2005},
  journal = {Neural networks},
  volume = {18},
  number = {5-6},
  pages = {602--610}
}

@incollection{graves_supervised_2012,
  ids = {gravesSupervisedSequenceLabelling2012},
  title = {Supervised Sequence Labelling},
  booktitle = {Supervised Sequence Labelling with Recurrent Neural Networks},
  author = {Graves, Alex},
  year = {2012},
  pages = {5--13},
  publisher = {{Springer}},
  file = {/Users/davidnicholson/Zotero/storage/6QP3Q8E5/Graves - 2012 - Supervised sequence labelling.pdf;/Users/davidnicholson/Zotero/storage/2IQVFCJL/978-3-642-24797-2_2.html}
}

@incollection{graves_supervised_2012,
  title = {Supervised Sequence Labelling},
  booktitle = {Supervised Sequence Labelling with Recurrent Neural Networks},
  author = {Graves, Alex},
  year = {2012},
  pages = {5--13},
  publisher = {{Springer}}
}

@inproceedings{graves2006connectionist,
  title = {Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks},
  booktitle = {Proceedings of the 23rd International Conference on {{Machine}} Learning},
  author = {Graves, Alex and Fern{\'a}ndez, Santiago and Gomez, Faustino and Schmidhuber, J{\"u}rgen},
  year = {2006},
  pages = {369--376}
}

@misc{griselScikitlearnScikitlearnScikitlearn2020,
  title = {Scikit-Learn/Scikit-Learn: Scikit-Learn 0.24.0},
  author = {Grisel, Olivier and Mueller, Andreas and {Lars} and Gramfort, Alexandre and Louppe, Gilles and Prettenhofer, Peter and Blondel, Mathieu and Niculae, Vlad and Nothman, Joel and Joly, Arnaud and Fan, Thomas J. and Vanderplas, Jake and {kumar}, manoj and Qin, Hanmin and Hug, Nicolas and Varoquaux, Nelle and Est{\`e}ve, Lo{\"i}c and Layton, Robert and Metzen, Jan Hendrik and Lemaitre, Guillaume and Dawe, Noel and Jalali, Adrin and (Venkat) Raghav, Rajagopalan and Sch{\"o}nberger, Johannes and Yurchak, Roman and Li, Wei and Woolam, Clay and {la Tour}, Tom Dupr{\'e} and Eren, Kemal and {du Boisberranger}, J{\'e}r{\'e}mie},
  year = {2020},
  month = dec,
  doi = {10.5281/zenodo.4385486},
  howpublished = {Zenodo}
}

@article{hahnloser_ultra-sparse_2002,
  title = {An Ultra-Sparse Code Underliesthe Generation of Neural Sequences in a Songbird},
  author = {Hahnloser, Richard HR and Kozhevnikov, Alexay A. and Fee, Michale S.},
  year = {2002},
  journal = {Nature},
  volume = {419},
  number = {6902},
  pages = {65--70}
}

@article{hahnloserUltrasparseCodeUnderliesthe2002,
  title = {An Ultra-Sparse Code Underliesthe Generation of Neural Sequences in a Songbird},
  author = {Hahnloser, Richard HR and Kozhevnikov, Alexay A. and Fee, Michale S.},
  year = {2002},
  journal = {Nature},
  volume = {419},
  number = {6902},
  pages = {65--70},
  file = {/Users/davidnicholson/Zotero/storage/UHPGDAF3/Hahnloser et al. - 2002 - An ultra-sparse code underliesthe generation of ne.pdf;/Users/davidnicholson/Zotero/storage/35FFRA2Z/nature00974.html}
}

@article{harris2020array,
  title = {Array Programming with {{NumPy}}},
  author = {Harris, Charles R and Millman, K Jarrod and {van der Walt}, St{\'e}fan J and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J and others},
  year = {2020},
  journal = {Nature},
  volume = {585},
  number = {7825},
  pages = {357--362},
  publisher = {{Nature Publishing Group}}
}

@article{heDeepResidualLearning2015,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  month = dec,
  journal = {arXiv:1512.03385 [cs]},
  eprint = {1512.03385},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers\textemdash 8\texttimes{} deeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/davidnicholson/Zotero/storage/3GZPPENK/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf}
}

@article{hedley2016complexity,
  title = {Complexity, Predictability and Time Homogeneity of Syntax in the Songs of {{Cassin}}'s Vireo ({{Vireo}} Cassinii)},
  author = {Hedley, Richard W},
  year = {2016},
  journal = {PloS one},
  volume = {11},
  number = {4},
  pages = {e0150822},
  publisher = {{Public Library of Science San Francisco, CA USA}}
}

@article{heusel2017gans,
  ids = {heuselGANsTrainedTwo},
  title = {Gans Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium},
  author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  year = {2017},
  journal = {Advances in neural information processing systems},
  volume = {30},
  file = {/Users/davidnicholson/Zotero/storage/KUJLZ5DN/Heusel et al. - GANs Trained by a Two Time-Scale Update Rule Conve.pdf}
}

@article{Hunter:2007,
  title = {Matplotlib: {{A 2D}} Graphics Environment},
  author = {Hunter, J. D.},
  year = {2007},
  journal = {Computing in Science \& Engineering},
  volume = {9},
  number = {3},
  pages = {90--95},
  publisher = {{IEEE COMPUTER SOC}},
  doi = {10.1109/MCSE.2007.55},
  abstract = {Matplotlib is a 2D graphics package used for Python for application development, interactive scripting, and publication-quality image generation across user interfaces and operating systems.}
}

@book{james2013introduction,
  title = {An Introduction to Statistical Learning},
  author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  year = {2013},
  volume = {112},
  publisher = {{Springer}}
}

@article{janikPitfallsCategorizationBehaviour1999,
  title = {Pitfalls in the Categorization of Behaviour: A Comparison of Dolphin Whistle Classification Methods},
  shorttitle = {Pitfalls in the Categorization of Behaviour},
  author = {Janik, Vincent M.},
  year = {1999},
  month = jan,
  journal = {Animal Behaviour},
  volume = {57},
  number = {1},
  pages = {133--143},
  issn = {00033472},
  doi = {10.1006/anbe.1998.0923},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/UQ48C9V2/Janik - 1999 - Pitfalls in the categorization of behaviour a com.pdf}
}

@article{jin2011compact,
  title = {A Compact Statistical Model of the Song Syntax in {{Bengalese}} Finch},
  author = {Jin, Dezhe Z and Kozhevnikov, Alexay A},
  year = {2011},
  journal = {PLoS computational biology},
  volume = {7},
  number = {3},
  pages = {e1001108},
  publisher = {{Public Library of Science}}
}

@article{kakishitaEthologicalDataMining2009,
  title = {Ethological Data Mining: An Automata-Based Approach to Extract Behavioral Units and Rules},
  shorttitle = {Ethological Data Mining},
  author = {Kakishita, Yasuki and Sasahara, Kazutoshi and Nishino, Tetsuro and Takahasi, Miki and Okanoya, Kazuo},
  year = {2009},
  month = jun,
  journal = {Data Mining and Knowledge Discovery},
  volume = {18},
  number = {3},
  pages = {446--471},
  issn = {1384-5810, 1573-756X},
  doi = {10.1007/s10618-008-0122-1},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/KG3P6WL5/Kakishita et al. - 2009 - Ethological data mining an automata-based approac.pdf}
}

@article{kershenbaumAcousticSequencesNonhuman2016,
  title = {Acoustic Sequences in Non-Human Animals: A Tutorial Review and Prospectus: {{Acoustic}} Sequences in Animals},
  shorttitle = {Acoustic Sequences in Non-Human Animals},
  author = {Kershenbaum, Arik and Blumstein, Daniel T. and Roch, Marie A. and Ak{\c c}ay, {\c C}a{\u g}lar and Backus, Gregory and Bee, Mark A. and Bohn, Kirsten and Cao, Yan and Carter, Gerald and C{\"a}sar, Cristiane and Coen, Michael and DeRuiter, Stacy L. and Doyle, Laurance and Edelman, Shimon and {Ferrer-i-Cancho}, Ramon and Freeberg, Todd M. and Garland, Ellen C. and Gustison, Morgan and Harley, Heidi E. and Huetz, Chlo{\'e} and Hughes, Melissa and Hyland Bruno, Julia and Ilany, Amiyaal and Jin, Dezhe Z. and Johnson, Michael and Ju, Chenghui and Karnowski, Jeremy and Lohr, Bernard and Manser, Marta B. and McCowan, Brenda and Mercado, Eduardo and Narins, Peter M. and Piel, Alex and Rice, Megan and Salmi, Roberta and Sasahara, Kazutoshi and Sayigh, Laela and Shiu, Yu and Taylor, Charles and Vallejo, Edgar E. and Waller, Sara and {Zamora-Gutierrez}, Veronica},
  year = {2016},
  month = feb,
  journal = {Biological Reviews},
  volume = {91},
  number = {1},
  pages = {13--52},
  issn = {14647931},
  doi = {10.1111/brv.12160},
  abstract = {Animal acoustic communication often takes the form of complex sequences, made up of multiple distinct acoustic units. Apart from the well-known example of birdsong, other animals such as insects, amphibians, and mammals (including bats, rodents, primates, and cetaceans) also generate complex acoustic sequences. Occasionally, such as with birdsong, the adaptive role of these sequences seems clear (e.g. mate attraction and territorial defence). More often however, researchers have only begun to characterise \textendash{} let alone understand \textendash{} the significance and meaning of acoustic sequences. Hypotheses abound, but there is little agreement as to how sequences should be defined and analysed. Our review aims to outline suitable methods for testing these hypotheses, and to describe the major limitations to our current and near-future knowledge on questions of acoustic sequences. This review and prospectus is the result of a collaborative effort between 43 scientists from the fields of animal behaviour, ecology and evolution, signal processing, machine learning, quantitative linguistics, and information theory, who gathered for a 2013 workshop entitled, `Analysing vocal sequences in animals'. Our goal is to present not just a review of the state of the art, but to propose a methodological framework that summarises what we suggest are the best practices for research in this field, across taxa and across disciplines. We also provide a tutorial-style introduction to some of the most promising algorithmic approaches for analysing sequences. We divide our review into three sections: identifying the distinct units of an acoustic sequence, describing the different ways that information can be contained within a sequence, and analysing the structure of that sequence. Each of these sections is further subdivided to address the key questions and approaches in that area. We propose a uniform, systematic, and comprehensive approach to studying sequences, with the goal of clarifying research terms used in different fields, and facilitating collaboration and comparative studies. Allowing greater interdisciplinary collaboration will facilitate the investigation of many important questions in the evolution of communication and sociality.},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/27GDVZ5V/Kershenbaum et al. - 2016 - Acoustic sequences in non-human animals a tutoria.pdf}
}

@article{kingmaAdamMethodStochastic2014,
  title = {Adam: {{A}} Method for Stochastic Optimization},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2014},
  journal = {arXiv preprint arXiv:1412.6980},
  eprint = {1412.6980},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {/Users/davidnicholson/Zotero/storage/IYFCSCAM/Kingma and Ba - 2014 - Adam A method for stochastic optimization.pdf;/Users/davidnicholson/Zotero/storage/7XAGPER8/1412.html}
}

@inproceedings{kluyver2016jupyter,
  title = {Jupyter {{Notebooks-a}} Publishing Format for Reproducible Computational Workflows.},
  booktitle = {{{ELPUB}}},
  author = {Kluyver, Thomas and {Ragan-Kelley}, Benjamin and P{\'e}rez, Fernando and Granger, Brian E and Bussonnier, Matthias and Frederic, Jonathan and Kelley, Kyle and Hamrick, Jessica B and Grout, Jason and Corlay, Sylvain and others},
  year = {2016},
  pages = {87--90}
}

@article{kogan1998automated,
  title = {Automated Recognition of Bird Song Elements from Continuous Recordings Using Dynamic Time Warping and Hidden {{Markov}} Models: {{A}} Comparative Study},
  author = {Kogan, Joseph A and Margoliash, Daniel},
  year = {1998},
  journal = {The Journal of the Acoustical Society of America},
  volume = {103},
  number = {4},
  pages = {2185--2196},
  publisher = {{Acoustical Society of America}}
}

@article{koumura_automatic_2016-1,
  ids = {koumuraAutomaticRecognitionElement2016,koumuraAutomaticRecognitionElement2016a,koumura_automatic_2016},
  title = {Automatic Recognition of Element Classes and Boundaries in the Birdsong with Variable Sequences},
  author = {Koumura, Takuya and Okanoya, Kazuo},
  year = {2016},
  journal = {PLoS ONE},
  volume = {11},
  number = {7},
  issn = {19326203},
  doi = {10.1371/journal.pone.0159188},
  abstract = {Researches on sequential vocalization often require analysis of vocalizations in long continuous sounds. In such studies as developmental ones or studies across generations in which days or months of vocalizations must be analyzed, methods for automatic recognition would be strongly desired. Although methods for automatic speech recognition for application purposes have been intensively studied, blindly applying them for biological purposes may not be an optimal solution. This is because, unlike human speech recognition, analysis of sequential vocalizations often requires accurate extraction of timing information. In the present study we propose automated systems suitable for recognizing birdsong, one of the most intensively investigated sequential vocalizations, focusing on the three properties of the birdsong. First, a song is a sequence of vocal elements, called notes, which can be grouped into categories. Second, temporal structure of birdsong is precisely controlled, meaning that temporal information is important in song analysis. Finally, notes are produced according to certain probabilistic rules, which may facilitate the accurate song recognition. We divided the procedure of song recognition into three sub-steps: local classification, boundary detection, and global sequencing, each of which corresponds to each of the three properties of birdsong. We compared the performances of several different ways to arrange these three steps. As results, we demonstrated a hybrid model of a deep convolutional neural network and a hidden Markov model was effective. We propose suitable arrangements of methods according to whether accurate boundary detection is needed. Also we designed the new measure to jointly evaluate the accuracy of note classification and boundary detection. Our methods should be applicable, with small modification and tuning, to the songs in other species that hold the three properties of the sequential vocalization.},
  keywords = {Bird song,Birds,Hidden Markov models,Markov models,Neural networks,speech,Syntax,vocalization},
  file = {/Users/davidnicholson/Zotero/storage/RCKYJPWR/Koumura and Okanoya - 2016 - Automatic Recognition of Element Classes and Bound.pdf;/Users/davidnicholson/Zotero/storage/WU5J3Y8V/article.html}
}

@article{koumura_automatic_2016-1,
  title = {Automatic Recognition of Element Classes and Boundaries in the Birdsong with Variable Sequences},
  author = {Koumura, Takuya and Okanoya, Kazuo},
  year = {2016},
  journal = {PLoS ONE},
  volume = {11},
  number = {7},
  issn = {19326203},
  doi = {10.1371/journal.pone.0159188},
  abstract = {Researches on sequential vocalization often require analysis of vocalizations in long continuous sounds. In such studies as developmental ones or studies across generations in which days or months of vocalizations must be analyzed, methods for automatic recognition would be strongly desired. Although methods for automatic speech recognition for application purposes have been intensively studied, blindly applying them for biological purposes may not be an optimal solution. This is because, unlike human speech recognition, analysis of sequential vocalizations often requires accurate extraction of timing information. In the present study we propose automated systems suitable for recognizing birdsong, one of the most intensively investigated sequential vocalizations, focusing on the three properties of the birdsong. First, a song is a sequence of vocal elements, called notes, which can be grouped into categories. Second, temporal structure of birdsong is precisely controlled, meaning that temporal information is important in song analysis. Finally, notes are produced according to certain probabilistic rules, which may facilitate the accurate song recognition. We divided the procedure of song recognition into three sub-steps: local classification, boundary detection, and global sequencing, each of which corresponds to each of the three properties of birdsong. We compared the performances of several different ways to arrange these three steps. As results, we demonstrated a hybrid model of a deep convolutional neural network and a hidden Markov model was effective. We propose suitable arrangements of methods according to whether accurate boundary detection is needed. Also we designed the new measure to jointly evaluate the accuracy of note classification and boundary detection. Our methods should be applicable, with small modification and tuning, to the songs in other species that hold the three properties of the sequential vocalization.},
  keywords = {Bird song,Birds,Hidden Markov models,Markov models,Neural networks,speech,Syntax,vocalization}
}

@article{koumura_birdsongrecognition_2016,
  title = {{{BirdsongRecognition}}},
  author = {Koumura, Takuya},
  year = {2016},
  month = jul,
  doi = {10.6084/m9.figshare.3470165.v1},
  abstract = {This is a test data for the program "Birdsong Recognition" (https://github.com/takuya-koumura/birdsong-recognition) and the manuscript "Automatic recognition of element classes and boundaries in the birdsong with variable sequences" by Takuya Koumura and Kazuo Okanoya (http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0159188).The source code of the program is available at https://github.com/takuya-koumura/birdsong-recognition.The data includes songs in Bengalese finches, manual annotations of the song elements, and expected validation errors.Songs were collected from eleven birds (from Bird0 to Bird10). Data is located in the directory ``Wave E The sound format is 16-bit linear PCM with sampling rate of 32 kHz.Annotations are defined in XML format. Annotation of each sequence includes the name of wave file, position and length of the sequence in the wave file, and annotations of sound elements in the sequence. Annotation of each sound element includes position and length in the sequence and the label of the element. The schema for the XML file is in birdsong-recognition/xsd/AnnotationSchema.xsd or at http://marler.c.u-tokyo.ac.jp/files/koumura-okanoya-2016-songs/xsd/AnnotationSchema.xsdValidation errors are the expected values that would be obtained by running the program in https://github.com/takuya-koumura/birdsong-recognition with hyper parameters provided in the source code. The program contains three algorithms for automatic recognition: "BD -{$>$} LC -{$>$} GS", "LC -{$>$} BD \& GS" and "LC \& GS -{$>$} BD \& GS" Expected errors for each algorithm are provided in "ErrorBdLcGs.xml", "ErrorLcBdGs.xml", "ErrorLcGsBdGs.xml". Each file contains two types of validation errors: "Levenshtein error" and "matching error". Please read the manuscript for the description of the algorithms and errors. The schema for the XML file is in birdsong-recognition/xsd/ErrorSchema.xsd or at http://marler.c.u-tokyo.ac.jp/files/koumura-okanoya-2016-songs/xsd/ErrorSchema.xsd},
  keywords = {birdsong}
}

@article{koumura_birdsongrecognition_2016,
  title = {{{BirdsongRecognition}}},
  author = {Koumura, Takuya},
  year = {2016},
  month = jul,
  doi = {10.6084/m9.figshare.3470165.v1},
  abstract = {This is a test data for the program "Birdsong Recognition" (https://github.com/takuya-koumura/birdsong-recognition) and the manuscript "Automatic recognition of element classes and boundaries in the birdsong with variable sequences" by Takuya Koumura and Kazuo Okanoya (http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0159188).The source code of the program is available at https://github.com/takuya-koumura/birdsong-recognition.The data includes songs in Bengalese finches, manual annotations of the song elements, and expected validation errors.Songs were collected from eleven birds (from Bird0 to Bird10). Data is located in the directory ``Wave E The sound format is 16-bit linear PCM with sampling rate of 32 kHz.Annotations are defined in XML format. Annotation of each sequence includes the name of wave file, position and length of the sequence in the wave file, and annotations of sound elements in the sequence. Annotation of each sound element includes position and length in the sequence and the label of the element. The schema for the XML file is in birdsong-recognition/xsd/AnnotationSchema.xsd or at http://marler.c.u-tokyo.ac.jp/files/koumura-okanoya-2016-songs/xsd/AnnotationSchema.xsdValidation errors are the expected values that would be obtained by running the program in https://github.com/takuya-koumura/birdsong-recognition with hyper parameters provided in the source code. The program contains three algorithms for automatic recognition: "BD -{$>$} LC -{$>$} GS", "LC -{$>$} BD \& GS" and "LC \& GS -{$>$} BD \& GS" Expected errors for each algorithm are provided in "ErrorBdLcGs.xml", "ErrorLcBdGs.xml", "ErrorLcGsBdGs.xml". Each file contains two types of validation errors: "Levenshtein error" and "matching error". Please read the manuscript for the description of the algorithms and errors. The schema for the XML file is in birdsong-recognition/xsd/ErrorSchema.xsd or at http://marler.c.u-tokyo.ac.jp/files/koumura-okanoya-2016-songs/xsd/ErrorSchema.xsd},
  keywords = {birdsong}
}

@incollection{krizhevsky_imagenet_2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 25},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
  year = {2012},
  pages = {1097--1105},
  publisher = {{Curran Associates, Inc.}}
}

@incollection{krizhevskyImageNetClassificationDeep2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 25},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
  year = {2012},
  pages = {1097--1105},
  publisher = {{Curran Associates, Inc.}},
  file = {/Users/davidnicholson/Zotero/storage/FDW9VVFI/Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf;/Users/davidnicholson/Zotero/storage/IUGRV5SK/4824-imagenet-classification-with-deep-convolutional-neural-networks.html}
}

@inproceedings{lea2017temporal,
  title = {Temporal Convolutional Networks for Action Segmentation and Detection},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Lea, Colin and Flynn, Michael D and Vidal, Rene and Reiter, Austin and Hager, Gregory D},
  year = {2017},
  pages = {156--165}
}

@inproceedings{lea2017temporal,
  title = {Temporal Convolutional Networks for Action Segmentation and Detection},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Lea, Colin and Flynn, Michael D and Vidal, Rene and Reiter, Austin and Hager, Gregory D},
  year = {2017},
  pages = {156--165}
}

@article{leonardoEnsembleCodingVocal2005,
  title = {Ensemble {{Coding}} of {{Vocal Control}} in {{Birdsong}}},
  author = {Leonardo, A.},
  year = {2005},
  month = jan,
  journal = {Journal of Neuroscience},
  volume = {25},
  number = {3},
  pages = {652--661},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3036-04.2005},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/YARJ3ATE/Leonardo - 2005 - Ensemble Coding of Vocal Control in Birdsong.pdf}
}

@article{liuPayAttentionMLPs2021,
  title = {Pay {{Attention}} to {{MLPs}}},
  author = {Liu, Hanxiao and Dai, Zihang and So, David R. and Le, Quoc V.},
  year = {2021},
  month = may,
  journal = {arXiv:2105.08050 [cs]},
  eprint = {2105.08050},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Transformers [1] have become one of the most important architectural innovations in deep learning and have enabled many breakthroughs over the past few years. Here we propose a simple attention-free network architecture, gMLP, based solely on MLPs with gating, and show that it can perform as well as Transformers in key language and vision applications. Our comparisons show that self-attention is not critical for Vision Transformers, as gMLP can achieve the same accuracy. For BERT, our model achieves parity with Transformers on pretraining perplexity and is better on some downstream tasks. On finetuning tasks where gMLP performs worse, making the gMLP model substantially larger can close the gap with Transformers. In general, our experiments show that gMLP can scale as well as Transformers over increased data and compute.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/davidnicholson/Zotero/storage/H9BWB39C/Liu et al. - 2021 - Pay Attention to MLPs.pdf}
}

@article{long_support_2010,
  title = {Support for a Synaptic Chain Model of Neuronal Sequence Generation},
  author = {Long, Michael A. and Jin, Dezhe Z. and Fee, Michale S.},
  year = {2010},
  journal = {Nature},
  volume = {468},
  number = {7322},
  pages = {394--399}
}

@article{long_using_2008,
  title = {Using Temperature to Analyse Temporal Dynamics in the Songbird Motor Pathway},
  author = {Long, Michael A. and Fee, Michale S.},
  year = {2008},
  journal = {Nature},
  volume = {456},
  number = {7219},
  pages = {189--194}
}

@article{longSupportSynapticChain2010,
  title = {Support for a Synaptic Chain Model of Neuronal Sequence Generation},
  author = {Long, Michael A. and Jin, Dezhe Z. and Fee, Michale S.},
  year = {2010},
  journal = {Nature},
  volume = {468},
  number = {7322},
  pages = {394--399},
  file = {/Users/davidnicholson/Zotero/storage/MQU7KBUV/PMC2998755.html;/Users/davidnicholson/Zotero/storage/NQPEE3KK/nature09514.html}
}

@article{longUsingTemperatureAnalyse2008,
  title = {Using Temperature to Analyse Temporal Dynamics in the Songbird Motor Pathway},
  author = {Long, Michael A. and Fee, Michale S.},
  year = {2008},
  journal = {Nature},
  volume = {456},
  number = {7219},
  pages = {189--194},
  file = {/Users/davidnicholson/Zotero/storage/AZMEE6PZ/PMC2723166.html;/Users/davidnicholson/Zotero/storage/RSBAI9IC/nature07448.html}
}

@inproceedings{marcel_torchvision_2010,
  title = {Torchvision the Machine-Vision Package of Torch},
  booktitle = {Proceedings of the 18th {{ACM}} International Conference on {{Multimedia}}},
  author = {Marcel, S{\'e}bastien and Rodriguez, Yann},
  year = {2010},
  month = oct,
  series = {{{MM}} '10},
  pages = {1485--1488},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1873951.1874254},
  abstract = {This paper presents Torchvision an open source machine vision package for Torch. Torch is a machine learning library providing a series of the state-of-the-art algorithms such as Neural Networks, Support Vector Machines, Gaussian Mixture Models, Hidden Markov Models and many others. Torchvision provides additional functionalities to manipulate and process images with standard image processing algorithms. Hence, the resulting images can be used directly with the Torch machine learning algorithms as Torchvision is fully integrated with Torch. Both Torch and Torchvision are written in C++ language and are publicly available under the Free-BSD License.},
  isbn = {978-1-60558-933-6}
}

@inproceedings{marcelTorchvisionMachinevisionPackage2010,
  title = {Torchvision the Machine-Vision Package of Torch},
  booktitle = {Proceedings of the 18th {{ACM}} International Conference on {{Multimedia}}},
  author = {Marcel, S{\'e}bastien and Rodriguez, Yann},
  year = {2010},
  month = oct,
  series = {{{MM}} '10},
  pages = {1485--1488},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1873951.1874254},
  abstract = {This paper presents Torchvision an open source machine vision package for Torch. Torch is a machine learning library providing a series of the state-of-the-art algorithms such as Neural Networks, Support Vector Machines, Gaussian Mixture Models, Hidden Markov Models and many others. Torchvision provides additional functionalities to manipulate and process images with standard image processing algorithms. Hence, the resulting images can be used directly with the Torch machine learning algorithms as Torchvision is fully integrated with Torch. Both Torch and Torchvision are written in C++ language and are publicly available under the Free-BSD License.},
  isbn = {978-1-60558-933-6}
}

@article{markowitz_long-range_2013,
  ids = {markowitzLongrangeOrderCanary2013},
  title = {Long-Range {{Order}} in {{Canary Song}}},
  author = {Markowitz, Jeffrey E. and Ivie, Elizabeth and Kligler, Laura and Gardner, Timothy J.},
  year = {2013},
  month = may,
  journal = {PLOS Computational Biology},
  volume = {9},
  number = {5},
  pages = {e1003052},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003052},
  abstract = {Bird songs range in form from the simple notes of a Chipping Sparrow to the rich performance of the nightingale. Non-adjacent correlations can be found in the syntax of some birdsongs, indicating that the choice of what to sing next is determined not only by the current syllable, but also by previous syllables sung. Here we examine the song of the domesticated canary, a complex singer whose song consists of syllables, grouped into phrases that are arranged in flexible sequences. Phrases are defined by a fundamental time-scale that is independent of the underlying syllable duration. We show that the ordering of phrases is governed by long-range rules: the choice of what phrase to sing next in a given context depends on the history of the song, and for some syllables, highly specific rules produce correlations in song over timescales of up to ten seconds. The neural basis of these long-range correlations may provide insight into how complex behaviors are assembled from more elementary, stereotyped modules.},
  langid = {english},
  keywords = {Acoustics,Bird song,Birds,Canaries,Entropy,Markov processes,Syllables,Syntax},
  file = {/Users/davidnicholson/Zotero/storage/4CQ69MCW/Markowitz et al. - 2013 - Long-range Order in Canary Song.pdf;/Users/davidnicholson/Zotero/storage/9BUEJ99D/article.html}
}

@article{markowitz_long-range_2013,
  title = {Long-Range {{Order}} in {{Canary Song}}},
  author = {Markowitz, Jeffrey E. and Ivie, Elizabeth and Kligler, Laura and Gardner, Timothy J.},
  year = {2013},
  month = may,
  journal = {PLOS Computational Biology},
  volume = {9},
  number = {5},
  pages = {e1003052},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003052},
  abstract = {Bird songs range in form from the simple notes of a Chipping Sparrow to the rich performance of the nightingale. Non-adjacent correlations can be found in the syntax of some birdsongs, indicating that the choice of what to sing next is determined not only by the current syllable, but also by previous syllables sung. Here we examine the song of the domesticated canary, a complex singer whose song consists of syllables, grouped into phrases that are arranged in flexible sequences. Phrases are defined by a fundamental time-scale that is independent of the underlying syllable duration. We show that the ordering of phrases is governed by long-range rules: the choice of what phrase to sing next in a given context depends on the history of the song, and for some syllables, highly specific rules produce correlations in song over timescales of up to ten seconds. The neural basis of these long-range correlations may provide insight into how complex behaviors are assembled from more elementary, stereotyped modules.},
  langid = {english},
  keywords = {Acoustics,Bird song,Birds,Canaries,Entropy,Markov processes,Syllables,Syntax}
}

@article{mathis_deeplabcut_2018,
  title = {{{DeepLabCut}}: Markerless Pose Estimation of User-Defined Body Parts with Deep Learning},
  shorttitle = {{{DeepLabCut}}},
  author = {Mathis, Alexander and Mamidanna, Pranav and Cury, Kevin M. and Abe, Taiga and Murthy, Venkatesh N. and Mathis, Mackenzie Weygandt and Bethge, Matthias},
  year = {2018},
  month = sep,
  journal = {Nature Neuroscience},
  volume = {21},
  number = {9},
  pages = {1281--1289},
  issn = {1546-1726},
  doi = {10.1038/s41593-018-0209-y},
  abstract = {Quantifying behavior is crucial for many applications in neuroscience. Videography provides easy methods for the observation and recording of animal behavior in diverse settings, yet extracting particular aspects of a behavior for further analysis can be highly time consuming. In motor control studies, humans or other animals are often marked with reflective markers to assist with computer-based tracking, but markers are intrusive, and the number and location of the markers must be determined a priori. Here we present an efficient method for markerless pose estimation based on transfer learning with deep neural networks that achieves excellent results with minimal training data. We demonstrate the versatility of this framework by tracking various body parts in multiple species across a broad collection of behaviors. Remarkably, even when only a small number of frames are labeled (\textasciitilde 200), the algorithm achieves excellent tracking performance on test frames that is comparable to human accuracy.},
  copyright = {2018 The Author(s)},
  langid = {english}
}

@article{mathisDeepLabCutMarkerlessPose2018,
  title = {{{DeepLabCut}}: Markerless Pose Estimation of User-Defined Body Parts with Deep Learning},
  shorttitle = {{{DeepLabCut}}},
  author = {Mathis, Alexander and Mamidanna, Pranav and Cury, Kevin M. and Abe, Taiga and Murthy, Venkatesh N. and Mathis, Mackenzie Weygandt and Bethge, Matthias},
  year = {2018},
  month = sep,
  journal = {Nature Neuroscience},
  volume = {21},
  number = {9},
  pages = {1281--1289},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/s41593-018-0209-y},
  abstract = {Quantifying behavior is crucial for many applications in neuroscience. Videography provides easy methods for the observation and recording of animal behavior in diverse settings, yet extracting particular aspects of a behavior for further analysis can be highly time consuming. In motor control studies, humans or other animals are often marked with reflective markers to assist with computer-based tracking, but markers are intrusive, and the number and location of the markers must be determined a priori. Here we present an efficient method for markerless pose estimation based on transfer learning with deep neural networks that achieves excellent results with minimal training data. We demonstrate the versatility of this framework by tracking various body parts in multiple species across a broad collection of behaviors. Remarkably, even when only a small number of frames are labeled (\textasciitilde 200), the algorithm achieves excellent tracking performance on test frames that is comparable to human accuracy.},
  copyright = {2018 The Author(s)},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/ZPS5I296/Mathis et al. - 2018 - DeepLabCut markerless pose estimation of user-def.pdf;/Users/davidnicholson/Zotero/storage/Y2CNWNYX/s41593-018-0209-y.html}
}

@inproceedings{mckinney_data_2010,
  title = {Data {{Structures}} for {{Statistical Computing}} in {{Python}}},
  booktitle = {Proceedings of the 9th {{Python}} in {{Science Conference}}},
  author = {McKinney, Wes},
  editor = {van der Walt, St{\'e}fan and Millman, Jarrod},
  year = {2010},
  pages = {56--61},
  doi = {10.25080/Majora-92bf1922-00a}
}

@inproceedings{mckinneyDataStructuresStatistical2010,
  title = {Data {{Structures}} for {{Statistical Computing}} in {{Python}}},
  booktitle = {Proceedings of the 9th {{Python}} in {{Science Conference}}},
  author = {McKinney, Wes},
  editor = {van der Walt, St{\'e}fan and Millman, Jarrod},
  year = {2010},
  pages = {56--61},
  doi = {10.25080/Majora-92bf1922-00a}
}

@article{mets_automated_2018,
  title = {An Automated Approach to the Quantitation of Vocalizations and Vocal Learning in the Songbird},
  author = {Mets, David G. and Brainard, Michael S.},
  year = {2018},
  journal = {PLoS computational biology},
  volume = {14},
  number = {8},
  pages = {e1006437}
}

@article{mets_genetic_2018,
  title = {Genetic Variation Interacts with Experience to Determine Interindividual Differences in Learned Song},
  author = {Mets, David G. and Brainard, Michael S.},
  year = {2018},
  journal = {Proceedings of the National Academy of Sciences},
  volume = {115},
  number = {2},
  pages = {421--426}
}

@article{mets_learning_2019,
  title = {Learning Is Enhanced by Tailoring Instruction to Individual Genetic Differences},
  author = {Mets, David G. and Brainard, Michael S.},
  year = {2019},
  journal = {eLife},
  volume = {8}
}

@article{metsAutomatedApproachQuantitation2018,
  title = {An Automated Approach to the Quantitation of Vocalizations and Vocal Learning in the Songbird},
  author = {Mets, David G. and Brainard, Michael S.},
  year = {2018},
  journal = {PLoS computational biology},
  volume = {14},
  number = {8},
  pages = {e1006437},
  file = {/Users/davidnicholson/Zotero/storage/58GGFZ95/article.html}
}

@article{metsGeneticVariationInteracts2018,
  title = {Genetic Variation Interacts with Experience to Determine Interindividual Differences in Learned Song},
  author = {Mets, David G. and Brainard, Michael S.},
  year = {2018},
  journal = {Proceedings of the National Academy of Sciences},
  volume = {115},
  number = {2},
  pages = {421--426},
  file = {/Users/davidnicholson/Zotero/storage/GNDQM9D5/Mets and Brainard - 2018 - Genetic variation interacts with experience to det.pdf;/Users/davidnicholson/Zotero/storage/S4L3XN3F/421.html}
}

@article{metsLearningEnhancedTailoring2019,
  title = {Learning Is Enhanced by Tailoring Instruction to Individual Genetic Differences},
  author = {Mets, David G. and Brainard, Michael S.},
  year = {2019},
  journal = {eLife},
  volume = {8},
  file = {/Users/davidnicholson/Zotero/storage/DFNK94BY/PMC6748825.html}
}

@misc{michael_waskom_2020_4019146,
  title = {Mwaskom/Seaborn: V0.11.0 ({{Sepetmber}} 2020)},
  author = {Waskom, Michael and Botvinnik, Olga and Gelbart, Maoz and Ostblom, Joel and Hobson, Paul and Lukauskas, Saulius and Gemperline, David C and Augspurger, Tom and Halchenko, Yaroslav and Warmenhoven, Jordi and Cole, John B. and {de Ruiter}, Julian and Vanderplas, Jake and Hoyer, Stephan and Pye, Cameron and Miles, Alistair and Swain, Corban and Meyer, Kyle and Martin, Marcel and Bachant, Pete and Quintero, Eric and Kunter, Gero and Villalba, Santi and {Brian} and Fitzgerald, Clark and Evans, C.G. and Williams, Mike Lee and O'Kane, Drew and Yarkoni, Tal and Brunner, Thomas},
  year = {2020},
  month = sep,
  doi = {10.5281/zenodo.4019146},
  howpublished = {Zenodo}
}

@article{mooney_neurobiology_2009,
  title = {Neurobiology of Song Learning},
  author = {Mooney, Richard},
  year = {2009},
  journal = {Current opinion in neurobiology},
  volume = {19},
  number = {6},
  pages = {654--660}
}

@article{mooneyNeurobiologySongLearning2009,
  title = {Neurobiology of Song Learning},
  author = {Mooney, Richard},
  year = {2009},
  journal = {Current opinion in neurobiology},
  volume = {19},
  number = {6},
  pages = {654--660},
  file = {/Users/davidnicholson/Zotero/storage/2TJNWXUN/PMC5066577.html;/Users/davidnicholson/Zotero/storage/PC5X7ASJ/S095943880900141X.html}
}

@article{nicholson_bengalese_2017,
  title = {Bengalese {{Finch}} Song Repository},
  author = {Nicholson, David and Queen, Jonah E. and Sober, Samuel J.},
  year = {2017},
  month = oct,
  doi = {10.6084/m9.figshare.4805749.v5},
  abstract = {This is a collection of song from four Bengalese finches recorded in the Sober lab at Emory University. The song has been hand-labeled by two of the authors. To make it easy to work with the dataset, we have created a Python package, "evfuncs", available at https://github.com/soberlab/evfuncs (Please see "References" section below for a direct link).How to work with the files is described on the README of that library, but we describe the types of files here briefly. The actual sound files have the extension .cbin and were created by an application that runs behavioral experiments and collects data called EvTAF. Each .cbin file has an associated .cbin.not.mat file that contains song syllable onsets, offsets, labels, etc., created by a GUI for song annotation called evsonganaly. Each .cbin file also has associated .tmp and .rec files, also created by EvTAF. Those files are not strictly required to work with this dataset but are included for completeness.We share this collection as a means of testing different machine learning algorithms for classifying the elements of birdsong, known as syllables. A Python package for that purpose, "hybrid-vocal-classifier", was developed in part using this dataset.To learn more about hybrid-vocal-classifier, please visit https://hybrid-vocal-classifier.readthedocs.io/en/latest/ (see "References" section below for a direct link).},
  keywords = {Bengalese finch,Bengalese Finch Birdsongs,Bengalese finch song,Keras,machine learning,neuroscience/behavioral neuroscience,NumPy,Python,scikit-learn,SciPy,songbird studies,songbirds}
}

@article{nicholson_bengalese_2017,
  title = {Bengalese {{Finch}} Song Repository},
  author = {Nicholson, David and Queen, Jonah E. and Sober, Samuel J.},
  year = {2017},
  month = oct,
  doi = {10.6084/m9.figshare.4805749.v5},
  abstract = {This is a collection of song from four Bengalese finches recorded in the Sober lab at Emory University. The song has been hand-labeled by two of the authors. To make it easy to work with the dataset, we have created a Python package, "evfuncs", available at https://github.com/soberlab/evfuncs (Please see "References" section below for a direct link).How to work with the files is described on the README of that library, but we describe the types of files here briefly. The actual sound files have the extension .cbin and were created by an application that runs behavioral experiments and collects data called EvTAF. Each .cbin file has an associated .cbin.not.mat file that contains song syllable onsets, offsets, labels, etc., created by a GUI for song annotation called evsonganaly. Each .cbin file also has associated .tmp and .rec files, also created by EvTAF. Those files are not strictly required to work with this dataset but are included for completeness.We share this collection as a means of testing different machine learning algorithms for classifying the elements of birdsong, known as syllables. A Python package for that purpose, "hybrid-vocal-classifier", was developed in part using this dataset.To learn more about hybrid-vocal-classifier, please visit https://hybrid-vocal-classifier.readthedocs.io/en/latest/ (see "References" section below for a direct link).},
  keywords = {Bengalese finch,Bengalese Finch Birdsongs,Bengalese finch song,Keras,machine learning,neuroscience/behavioral neuroscience,NumPy,Python,scikit-learn,SciPy,songbird studies,songbirds}
}

@misc{Nicholson_crowsetta_2021,
  title = {Crowsetta},
  author = {Nicholson, David},
  year = {2021},
  month = mar,
  copyright = {BSD-3-Clause}
}

@misc{Nicholson_hybrid-vocal-classifier_2021,
  title = {Hybrid-Vocal-Classifier},
  author = {Nicholson, David},
  year = {2021},
  month = dec,
  copyright = {BSD-3-Clause}
}

@misc{Nicholson_vak_2021,
  title = {Vak},
  author = {Nicholson, David and Cohen, Yarden},
  year = {2021},
  month = nov,
  copyright = {BSD-3-Clause}
}

@inproceedings{nicholson2016comparison,
  title = {Comparison of Machine Learning Methods Applied to Birdsong Element Classification},
  booktitle = {Proceedings of the 15th Python in Science Conference},
  author = {Nicholson, David},
  year = {2016},
  pages = {57--61}
}

@techreport{nikolausLargescaleStudySpeech2021,
  type = {Preprint},
  title = {Large-Scale Study of Speech Acts' Development Using Automatic Labelling},
  author = {Nikolaus, Mitja and Maes, Juliette and Auguste, Jeremy and Pr{\'e}vot, Laurent and Fourtassi, Abdellah},
  year = {2021},
  month = may,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/j4smd},
  abstract = {Studies of children's language use in the wild (e.g., in the context of child-caregiver social interaction) have been slowed by the time- and resource- consuming task of hand annotating utterances for communicative intents/speech acts. Existing studies have typically focused on investigating rather small samples of children, raising the question of how their findings generalize both to larger and more representative populations and to a richer set of interaction contexts. Here we propose a simple automatic model for speech act labeling in early childhood based on the INCA-A coding scheme (Ninio et al., 1994). After validating the model against ground truth labels, we automatically annotated the entire English-language data from the CHILDES corpus. The major theoretical result was that earlier findings generalize quite well at a large scale. Our model will be shared with the community so that researchers can use it with their data to investigate various question related to language use both in typical and atypical populations of children.},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/4FA9PG9K/Nikolaus et al. - 2021 - Large-scale study of speech acts' development usin.pdf}
}

@article{nottebohmBrainAllSeasons1981,
  title = {A {{Brain}} for {{All Seasons}}: {{Cyclical Anatomical Changes}} in {{Song Control Nuclei}} of the {{Canary Brain}}},
  shorttitle = {A {{Brain}} for {{All Seasons}}},
  author = {Nottebohm, Fernando},
  year = {1981},
  month = dec,
  journal = {Science},
  volume = {214},
  number = {4527},
  pages = {1368--1370},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.7313697},
  file = {/Users/davidnicholson/Zotero/storage/G4DP3DUI/Nottebohm - 1981 - A Brain for All Seasons Cyclical Anatomical Chang.pdf}
}

@book{oliphant_guide_2015,
  title = {Guide to {{NumPy}}},
  author = {Oliphant, Travis E.},
  year = {2015},
  edition = {Second},
  publisher = {{CreateSpace Independent Publishing Platform}},
  address = {{North Charleston, SC, USA}},
  abstract = {This is the second edition of Travis Oliphant's A Guide to NumPy originally published electronically in 2006. It is designed to be a reference that can be used by practitioners who are familiar with Python but want to learn more about NumPy and related tools. In this updated edition, new perspectives are shared as well as descriptions of new distributed processing tools in the ecosystem, and how Numba can be used to compile code using NumPy arrays. Travis Oliphant is the co-founder and CEO of Continuum Analytics. Continuum Analytics develops Anaconda, the leading modern open source analytics platform powered by Python. Travis, who is a passionate advocate of open source technology, has a Ph.D. from Mayo Clinic and B.S. and M.S. degrees in Mathematics and Electrical Engineering from Brigham Young University. Since 1997, he has worked extensively with Python for computational and data science. He was the primary creator of the NumPy package and founding contributor to the SciPy package. He was also a co-founder and past board member of NumFOCUS, a non-profit for reproducible and accessible science that supports the PyData stack. He also served on the board of the Python Software Foundation.},
  isbn = {978-1-5173-0007-4}
}

@book{oliphantGuideNumPy2015,
  title = {Guide to {{NumPy}}},
  author = {Oliphant, Travis E.},
  year = {2015},
  edition = {Second},
  publisher = {{CreateSpace Independent Publishing Platform}},
  address = {{North Charleston, SC, USA}},
  abstract = {This is the second edition of Travis Oliphant's A Guide to NumPy originally published electronically in 2006. It is designed to be a reference that can be used by practitioners who are familiar with Python but want to learn more about NumPy and related tools. In this updated edition, new perspectives are shared as well as descriptions of new distributed processing tools in the ecosystem, and how Numba can be used to compile code using NumPy arrays. Travis Oliphant is the co-founder and CEO of Continuum Analytics. Continuum Analytics develops Anaconda, the leading modern open source analytics platform powered by Python. Travis, who is a passionate advocate of open source technology, has a Ph.D. from Mayo Clinic and B.S. and M.S. degrees in Mathematics and Electrical Engineering from Brigham Young University. Since 1997, he has worked extensively with Python for computational and data science. He was the primary creator of the NumPy package and founding contributor to the SciPy package. He was also a co-founder and past board member of NumFOCUS, a non-profit for reproducible and accessible science that supports the PyData stack. He also served on the board of the Python Software Foundation.},
  isbn = {978-1-5173-0007-4}
}

@article{olveczky_vocal_2005,
  title = {Vocal Experimentation in the Juvenile Songbird Requires a Basal Ganglia Circuit.},
  author = {Olveczky, Bence P. and Andalman, Aaron S. and Fee, Michale S.},
  year = {2005},
  journal = {PLoS biology},
  volume = {3},
  number = {5},
  pages = {e153--e153}
}

@article{olveczkyVocalExperimentationJuvenile2005,
  title = {Vocal Experimentation in the Juvenile Songbird Requires a Basal Ganglia Circuit.},
  author = {Olveczky, Bence P. and Andalman, Aaron S. and Fee, Michale S.},
  year = {2005},
  journal = {PLoS biology},
  volume = {3},
  number = {5},
  pages = {e153--e153},
  file = {/Users/davidnicholson/Zotero/storage/WIHLW6LK/Olveczky et al. - 2005 - Vocal experimentation in the juvenile songbird req.pdf}
}

@article{otchy_acute_2015,
  title = {Acute Off-Target Effects of Neural Circuit Manipulations},
  author = {Otchy, Timothy M. and Wolff, Steffen B. E. and Rhee, Juliana Y. and Pehlevan, Cengiz and Kawai, Risa and Kempf, Alexandre and Gobes, Sharon M. H. and {\"O}lveczky, Bence P.},
  year = {2015},
  month = dec,
  journal = {Nature},
  volume = {528},
  number = {7582},
  pages = {358--363},
  issn = {1476-4687},
  doi = {10.1038/nature16442},
  abstract = {Rapid and reversible manipulations of neural activity in behaving animals are transforming our understanding of brain function. An important assumption underlying much of this work is that evoked behavioural changes reflect the function of the manipulated circuits. We show that this assumption is problematic because it disregards indirect effects on the independent functions of downstream circuits. Transient inactivations of motor cortex in rats and nucleus interface (Nif) in songbirds severely degraded task-specific movement patterns and courtship songs, respectively, which are learned skills that recover spontaneously after permanent lesions of the same areas. We resolve this discrepancy in songbirds, showing that Nif silencing acutely affects the function of HVC, a downstream song control nucleus. Paralleling song recovery, the off-target effects resolved within days of Nif lesions, a recovery consistent with homeostatic regulation of neural activity in HVC. These results have implications for interpreting transient circuit manipulations and for understanding recovery after brain lesions.},
  copyright = {2015 Nature Publishing Group},
  langid = {english}
}

@article{otchyAcuteOfftargetEffects2015,
  ids = {otchy_acute_2015},
  title = {Acute Off-Target Effects of Neural Circuit Manipulations},
  author = {Otchy, Timothy M. and Wolff, Steffen B. E. and Rhee, Juliana Y. and Pehlevan, Cengiz and Kawai, Risa and Kempf, Alexandre and Gobes, Sharon M. H. and {\"O}lveczky, Bence P.},
  year = {2015},
  month = dec,
  journal = {Nature},
  volume = {528},
  number = {7582},
  pages = {358--363},
  issn = {1476-4687},
  doi = {10.1038/nature16442},
  abstract = {Rapid and reversible manipulations of neural activity in behaving animals are transforming our understanding of brain function. An important assumption underlying much of this work is that evoked behavioural changes reflect the function of the manipulated circuits. We show that this assumption is problematic because it disregards indirect effects on the independent functions of downstream circuits. Transient inactivations of motor cortex in rats and nucleus interface (Nif) in songbirds severely degraded task-specific movement patterns and courtship songs, respectively, which are learned skills that recover spontaneously after permanent lesions of the same areas. We resolve this discrepancy in songbirds, showing that Nif silencing acutely affects the function of HVC, a downstream song control nucleus. Paralleling song recovery, the off-target effects resolved within days of Nif lesions, a recovery consistent with homeostatic regulation of neural activity in HVC. These results have implications for interpreting transient circuit manipulations and for understanding recovery after brain lesions.},
  copyright = {2015 Nature Publishing Group},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/Y9SXPXMT/Otchy et al. - 2015 - Acute off-target effects of neural circuit manipul.pdf;/Users/davidnicholson/Zotero/storage/589D78M6/nature16442.html}
}

@inproceedings{ozaC2AEClassConditioned2019,
  title = {{{C2AE}}: {{Class Conditioned Auto-Encoder}} for {{Open-Set Recognition}}},
  shorttitle = {{{C2AE}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Oza, Poojan and Patel, Vishal M.},
  year = {2019},
  month = jun,
  pages = {2302--2311},
  publisher = {{IEEE}},
  address = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.00241},
  abstract = {Models trained for classification often assume that all testing classes are known while training. As a result, when presented with an unknown class during testing, such closed-set assumption forces the model to classify it as one of the known classes. However, in a real world scenario, classification models are likely to encounter such examples. Hence, identifying those examples as unknown becomes critical to model performance. A potential solution to overcome this problem lies in a class of learning problems known as open-set recognition. It refers to the problem of identifying the unknown classes during testing, while maintaining performance on the known classes. In this paper, we propose an open-set recognition algorithm using class conditioned auto-encoders with novel training and testing methodologies. In this method, training procedure is divided in two sub-tasks, 1. closed-set classification and, 2. open-set identification (i.e. identifying a class as known or unknown). Encoder learns the first task following the closed-set classification training pipeline, whereas decoder learns the second task by reconstructing conditioned on class identity. Furthermore, we model reconstruction errors using the Extreme Value Theory of statistical modeling to find the threshold for identifying known/unknown class samples. Experiments performed on multiple image classification datasets show that the proposed method performs significantly better than the state of the art methods. The source code is available at: github.com/otkupjnoz/c2ae.},
  isbn = {978-1-72813-293-8},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/8VEC9XST/Oza and Patel - 2019 - C2AE Class Conditioned Auto-Encoder for Open-Set .pdf}
}

@article{parascandolo_recurrent_2016,
  ids = {parascandoloRecurrentNeuralNetworks2016},
  title = {Recurrent {{Neural Networks}} for {{Polyphonic Sound Event Detection}} in {{Real Life Recordings}}},
  author = {Parascandolo, Giambattista and Huttunen, Heikki and Virtanen, Tuomas},
  year = {2016},
  month = mar,
  journal = {2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  eprint = {1604.00861},
  eprinttype = {arxiv},
  pages = {6440--6444},
  doi = {10.1109/ICASSP.2016.7472917},
  abstract = {In this paper we present an approach to polyphonic sound event detection in real life recordings based on bi-directional long short term memory (BLSTM) recurrent neural networks (RNNs). A single multilabel BLSTM RNN is trained to map acoustic features of a mixture signal consisting of sounds from multiple classes, to binary activity indicators of each event class. Our method is tested on a large database of real-life recordings, with 61 classes (e.g. music, car, speech) from 10 different everyday contexts. The proposed method outperforms previous approaches by a large margin, and the results are further improved using data augmentation techniques. Overall, our system reports an average F1-score of 65.5\% on 1 second blocks and 64.7\% on single frames, a relative improvement over previous state-of-the-art approach of 6.8\% and 15.1\% respectively.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Sound},
  file = {/Users/davidnicholson/Zotero/storage/YDCMEXJC/Parascandolo et al. - 2016 - Recurrent Neural Networks for Polyphonic Sound Eve.pdf;/Users/davidnicholson/Zotero/storage/SSMCAN93/1604.html}
}

@article{parascandolo_recurrent_2016,
  title = {Recurrent {{Neural Networks}} for {{Polyphonic Sound Event Detection}} in {{Real Life Recordings}}},
  author = {Parascandolo, Giambattista and Huttunen, Heikki and Virtanen, Tuomas},
  year = {2016},
  month = mar,
  journal = {2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages = {6440--6444},
  doi = {10.1109/ICASSP.2016.7472917},
  abstract = {In this paper we present an approach to polyphonic sound event detection in real life recordings based on bi-directional long short term memory (BLSTM) recurrent neural networks (RNNs). A single multilabel BLSTM RNN is trained to map acoustic features of a mixture signal consisting of sounds from multiple classes, to binary activity indicators of each event class. Our method is tested on a large database of real-life recordings, with 61 classes (e.g. music, car, speech) from 10 different everyday contexts. The proposed method outperforms previous approaches by a large margin, and the results are further improved using data augmentation techniques. Overall, our system reports an average F1-score of 65.5\% on 1 second blocks and 64.7\% on single frames, a relative improvement over previous state-of-the-art approach of 6.8\% and 15.1\% respectively.},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Sound}
}

@article{paszke_automatic_2017,
  title = {Automatic Differentiation in {{PyTorch}}},
  author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  year = {2017},
  month = oct,
  abstract = {A summary of automatic differentiation techniques employed in PyTorch library, including novelties like support for in-place modification in presence of objects aliasing the same data, performance...}
}

@article{paszkeAutomaticDifferentiationPyTorch2017,
  title = {Automatic Differentiation in {{PyTorch}}},
  author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  year = {2017},
  month = oct,
  abstract = {A summary of automatic differentiation techniques employed in PyTorch library, including novelties like support for in-place modification in presence of objects aliasing the same data, performance...},
  file = {/Users/davidnicholson/Zotero/storage/D4IFCXFP/Paszke et al. - 2017 - Automatic differentiation in PyTorch.pdf;/Users/davidnicholson/Zotero/storage/8NUQP4CC/forum.html}
}

@misc{paulboersmaPraatDoingPhonetics2021,
  title = {Praat: Doing Phonetics by Computer},
  author = {{Paul Boersma} and {David Weenink}},
  year = {2021}
}

@article{pearre_fast_2017,
  title = {A Fast and Accurate Zebra Finch Syllable Detector},
  author = {Pearre, Ben and Perkins, L. Nathan and Markowitz, Jeffrey E. and Gardner, Timothy J.},
  year = {2017},
  month = jul,
  journal = {PLOS ONE},
  volume = {12},
  number = {7},
  pages = {e0181992},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0181992},
  abstract = {The song of the adult male zebra finch is strikingly stereotyped. Efforts to understand motor output, pattern generation, and learning have taken advantage of this consistency by investigating the bird's ability to modify specific parts of song under external cues, and by examining timing relationships between neural activity and vocal output. Such experiments require that precise moments during song be identified in real time as the bird sings. Various syllable-detection methods exist, but many require special hardware, software, and know-how, and details on their implementation and performance are scarce. We present an accurate, versatile, and fast syllable detector that can control hardware at precisely timed moments during zebra finch song. Many moments during song can be isolated and detected with false negative and false positive rates well under 1\% and 0.005\% respectively. The detector can run on a stock Mac Mini with triggering delay of less than a millisecond and a jitter of {$\sigma$} {$\approx$} 2 milliseconds.},
  langid = {english}
}

@article{pearreFastAccurateZebra2017,
  title = {A Fast and Accurate Zebra Finch Syllable Detector},
  author = {Pearre, Ben and Perkins, L. Nathan and Markowitz, Jeffrey E. and Gardner, Timothy J.},
  year = {2017},
  month = jul,
  journal = {PLOS ONE},
  volume = {12},
  number = {7},
  pages = {e0181992},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0181992},
  abstract = {The song of the adult male zebra finch is strikingly stereotyped. Efforts to understand motor output, pattern generation, and learning have taken advantage of this consistency by investigating the bird's ability to modify specific parts of song under external cues, and by examining timing relationships between neural activity and vocal output. Such experiments require that precise moments during song be identified in real time as the bird sings. Various syllable-detection methods exist, but many require special hardware, software, and know-how, and details on their implementation and performance are scarce. We present an accurate, versatile, and fast syllable detector that can control hardware at precisely timed moments during zebra finch song. Many moments during song can be isolated and detected with false negative and false positive rates well under 1\% and 0.005\% respectively. The detector can run on a stock Mac Mini with triggering delay of less than a millisecond and a jitter of {$\sigma$} {$\approx$} 2 milliseconds.},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/WQ6PCLIW/Pearre et al. - 2017 - A fast and accurate zebra finch syllable detector.pdf;/Users/davidnicholson/Zotero/storage/Q3DSYFSH/comments.html}
}

@article{prat2017annotated,
  title = {An Annotated Dataset of {{Egyptian}} Fruit Bat Vocalizations across Varying Contexts and during Vocal Ontogeny},
  author = {Prat, Yosef and Taub, Mor and Pratt, Ester and Yovel, Yossi},
  year = {2017},
  journal = {Scientific data},
  volume = {4},
  number = {1},
  pages = {1--7},
  publisher = {{Nature Publishing Group}}
}

@misc{QuantifyingSimilarityAnimal,
  title = {‪{{Quantifying}} Similarity in Animal Vocal Sequences: {{Which}} Metric Performs Best?‬},
  shorttitle = {‪{{Quantifying}} Similarity in Animal Vocal Sequences},
  abstract = {‪A Kershenbaum, EC Garland‬, ‪Methods in Ecology and Evolution, 2015‬ - ‪Cited by 19‬},
  howpublished = {https://scholar.google.com/citations?view\_op=view\_citation\&hl=en\&user=aM-6g3kAAAAJ\&citation\_for\_view=aM-6g3kAAAAJ:v6i8RKmR8ToC},
  file = {/Users/davidnicholson/Zotero/storage/8H4H5R8L/citations.html}
}

@misc{ReducingNetworkAgnostophobia2021,
  title = {Reducing {{Network Agnostophobia}}},
  year = {2021},
  month = jul,
  howpublished = {Vision and Security Technology Lab}
}

@article{ron_power_1996,
  ids = {ronPowerAmnesiaLearning1996},
  title = {The Power of Amnesia: {{Learning}} Probabilistic Automata with Variable Memory Length},
  shorttitle = {The Power of Amnesia},
  author = {Ron, Dana and Singer, Yoram and Tishby, Naftali},
  year = {1996},
  month = nov,
  journal = {Machine Learning},
  volume = {25},
  number = {2},
  pages = {117--149},
  issn = {1573-0565},
  doi = {10.1007/BF00114008},
  abstract = {We propose and analyze a distribution learning algorithm for variable memory length Markov processes. These processes can be described by a subclass of probabilistic finite automata which we name Probabilistic Suffix Automata (PSA). Though hardness results are known for learning distributions generated by general probabilistic automata, we prove that the algorithm we present can efficiently learn distributions generated by PSAs. In particular, we show that for any target PSA, the KL-divergence between the distribution generated by the target and the distribution generated by the hypothesis the learning algorithm outputs, can be made small with high confidence in polynomial time and sample complexity. The learning algorithm is motivated by applications in human-machine interaction. Here we present two applications of the algorithm. In the first one we apply the algorithm in order to construct a model of the English language, and use this model to correct corrupted text. In the second application we construct a simple stochastic model for E.coli DNA.},
  langid = {english},
  keywords = {Learning distributions,Markov models,probabilistic automata,suffix trees,text correction},
  file = {/Users/davidnicholson/Zotero/storage/P4IH6U6C/Ron et al. - 1996 - The power of amnesia Learning probabilistic autom.pdf}
}

@article{ron_power_1996,
  title = {The Power of Amnesia: {{Learning}} Probabilistic Automata with Variable Memory Length},
  shorttitle = {The Power of Amnesia},
  author = {Ron, Dana and Singer, Yoram and Tishby, Naftali},
  year = {1996},
  month = nov,
  journal = {Machine Learning},
  volume = {25},
  number = {2},
  pages = {117--149},
  issn = {1573-0565},
  doi = {10.1007/BF00114008},
  abstract = {We propose and analyze a distribution learning algorithm for variable memory length Markov processes. These processes can be described by a subclass of probabilistic finite automata which we name Probabilistic Suffix Automata (PSA). Though hardness results are known for learning distributions generated by general probabilistic automata, we prove that the algorithm we present can efficiently learn distributions generated by PSAs. In particular, we show that for any target PSA, the KL-divergence between the distribution generated by the target and the distribution generated by the hypothesis the learning algorithm outputs, can be made small with high confidence in polynomial time and sample complexity. The learning algorithm is motivated by applications in human-machine interaction. Here we present two applications of the algorithm. In the first one we apply the algorithm in order to construct a model of the English language, and use this model to correct corrupted text. In the second application we construct a simple stochastic model for E.coli DNA.},
  langid = {english},
  keywords = {Learning distributions,Markov models,probabilistic automata,suffix trees,text correction}
}

@article{ruddExtremeValueMachine2018,
  title = {The {{Extreme Value Machine}}},
  author = {Rudd, Ethan M. and Jain, Lalit P. and Scheirer, Walter J. and Boult, Terrance E.},
  year = {2018},
  month = mar,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {40},
  number = {3},
  pages = {762--768},
  issn = {0162-8828, 2160-9292},
  doi = {10.1109/TPAMI.2017.2707495},
  abstract = {It is often desirable to be able to recognize when inputs to a recognition function learned in a supervised manner correspond to classes unseen at training time. With this ability, new class labels could be assigned to these inputs by a human operator, allowing them to be incorporated into the recognition function\textemdash ideally under an efficient incremental update mechanism. While good algorithms that assume inputs from a fixed set of classes exist, e.g., artificial neural networks and kernel machines, it is not immediately obvious how to extend them to perform incremental learning in the presence of unknown query classes. Existing algorithms take little to no distributional information into account when learning recognition functions and lack a strong theoretical foundation. We address this gap by formulating a novel, theoretically sound classifier\textemdash the Extreme Value Machine (EVM). The EVM has a well-grounded interpretation derived from statistical Extreme Value Theory (EVT), and is the first classifier to be able to perform nonlinear kernel-free variable bandwidth incremental learning. Compared to other classifiers in the same deep network derived feature space, the EVM is accurate and efficient on an established benchmark partition of the ImageNet dataset.},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/UGR4NC5T/Rudd et al. - 2018 - The Extreme Value Machine.pdf}
}

@inproceedings{sainath2013deep,
  title = {Deep Convolutional Neural Networks for {{LVCSR}}},
  booktitle = {2013 {{IEEE}} International Conference on Acoustics, Speech and Signal Processing},
  author = {Sainath, Tara N and Mohamed, Abdel-rahman and Kingsbury, Brian and Ramabhadran, Bhuvana},
  year = {2013},
  pages = {8614--8618},
  organization = {{IEEE}}
}

@inproceedings{sainath2013improvements,
  title = {Improvements to Deep Convolutional Neural Networks for {{LVCSR}}},
  booktitle = {2013 {{IEEE}} Workshop on Automatic Speech Recognition and Understanding},
  author = {Sainath, Tara N and Kingsbury, Brian and Mohamed, Abdel-rahman and Dahl, George E and Saon, George and Soltau, Hagen and Beran, Tomas and Aravkin, Aleksandr Y and Ramabhadran, Bhuvana},
  year = {2013},
  pages = {315--320},
  organization = {{IEEE}}
}

@article{sainathImprovementsDeepConvolutional2013,
  title = {Improvements to Deep Convolutional Neural Networks for {{LVCSR}}},
  author = {Sainath, Tara N. and Kingsbury, Brian and Mohamed, Abdel-rahman and Dahl, George E. and Saon, George and Soltau, Hagen and Beran, Tomas and Aravkin, Aleksandr Y. and Ramabhadran, Bhuvana},
  year = {2013},
  month = dec,
  journal = {arXiv:1309.1501 [cs, math, stat]},
  eprint = {1309.1501},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {Deep Convolutional Neural Networks (CNNs) are more powerful than Deep Neural Networks (DNN), as they are able to better reduce spectral variation in the input signal. This has also been confirmed experimentally, with CNNs showing improvements in word error rate (WER) between 4-12\% relative compared to DNNs across a variety of LVCSR tasks. In this paper, we describe different methods to further improve CNN performance. First, we conduct a deep analysis comparing limited weight sharing and full weight sharing with state-of-the-art features. Second, we apply various pooling strategies that have shown improvements in computer vision to an LVCSR speech task. Third, we introduce a method to effectively incorporate speaker adaptation, namely fMLLR, into log-mel features. Fourth, we introduce an effective strategy to use dropout during Hessian-free sequence training. We find that with these improvements, particularly with fMLLR and dropout, we are able to achieve an additional 2-3\% relative improvement in WER on a 50-hour Broadcast News task over our previous best CNN baseline. On a larger 400-hour BN task, we find an additional 4-5\% relative improvement over our previous best CNN baseline.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {65K05; 90C15; 90C90,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/davidnicholson/Zotero/storage/ZAKZG4P5/Sainath et al. - 2013 - Improvements to deep convolutional neural networks.pdf}
}

@inproceedings{sainburg2019animal,
  title = {Animal {{Vocalization Generative Network}} ({{AVGN}}): {{A}} Method for Visualizing, Understanding, and Sampling from Animal Communicative Repertoires.},
  booktitle = {{{CogSci}}},
  author = {Sainburg, Tim and Thielk, Marvin and Gentner, Timothy},
  year = {2019},
  pages = {3563}
}

@article{sainburg2019latent,
  title = {Latent Space Visualization, Characterization, and Generation of Diverse Vocal Communication Signals},
  author = {Sainburg, Tim and Thielk, Marvin and Gentner, Timothy Q},
  year = {2019},
  journal = {bioRxiv : the preprint server for biology},
  pages = {870311},
  publisher = {{Cold Spring Harbor Laboratory}}
}

@inproceedings{sainburgAnimalVocalizationGenerative2019,
  title = {Animal {{Vocalization Generative Network}} ({{AVGN}}): {{A}} Method for Visualizing, Understanding, and Sampling from Animal Communicative Repertoires.},
  shorttitle = {Animal {{Vocalization Generative Network}} ({{AVGN}})},
  booktitle = {{{CogSci}}},
  author = {Sainburg, Tim and Thielk, Marvin and Gentner, Timothy},
  year = {2019},
  pages = {3563}
}

@article{sainburgFindingVisualizingQuantifying2020,
  title = {Finding, Visualizing, and Quantifying Latent Structure across Diverse Animal Vocal Repertoires},
  author = {Sainburg, Tim and Thielk, Marvin and Gentner, Timothy Q.},
  editor = {Theunissen, Fr{\'e}d{\'e}ric E.},
  year = {2020},
  month = oct,
  journal = {PLOS Computational Biology},
  volume = {16},
  number = {10},
  pages = {e1008228},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1008228},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/FU2PPCDZ/Sainburg et al. - 2020 - Finding, visualizing, and quantifying latent struc.pdf}
}

@article{sainburgParallelsSequentialOrganization2019,
  title = {Parallels in the Sequential Organization of Birdsong and Human Speech},
  author = {Sainburg, Tim and Theilman, Brad and Thielk, Marvin and Gentner, Timothy Q.},
  year = {2019},
  month = dec,
  journal = {Nature Communications},
  volume = {10},
  number = {1},
  pages = {3636},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-11605-y},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/MHQMWUSJ/Sainburg et al. - 2019 - Parallels in the sequential organization of birdso.pdf}
}

@article{salimans2016improved,
  ids = {salimansImprovedTechniquesTraining},
  title = {Improved Techniques for Training Gans},
  author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
  year = {2016},
  journal = {Advances in neural information processing systems},
  volume = {29},
  pages = {2234--2242},
  file = {/Users/davidnicholson/Zotero/storage/G37GR7FE/Salimans et al. - Improved Techniques for Training GANs.pdf}
}

@article{scheirerMetaRecognitionTheoryPractice2011,
  title = {Meta-{{Recognition}}: {{The Theory}} and {{Practice}} of {{Recognition Score Analysis}}},
  shorttitle = {Meta-{{Recognition}}},
  author = {Scheirer, W J and Rocha, A and Micheals, R J and Boult, T E},
  year = {2011},
  month = aug,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {33},
  number = {8},
  pages = {1689--1695},
  issn = {0162-8828, 2160-9292},
  doi = {10.1109/TPAMI.2011.54},
  abstract = {In this paper, we define meta-recognition, a performance prediction method for recognition algorithms, and examine the theoretical basis for its postrecognition score analysis form through the use of the statistical extreme value theory (EVT). The ability to predict the performance of a recognition system based on its outputs for each match instance is desirable for a number of important reasons, including automatic threshold selection for determining matches and nonmatches, and automatic algorithm selection or weighting for multi-algorithm fusion. The emerging body of literature on postrecognition score analysis has been largely constrained to biometrics, where the analysis has been shown to successfully complement or replace image quality metrics as a predictor. We develop a new statistical predictor based upon the Weibull distribution, which produces accurate results on a per instance recognition basis across different recognition problems. Experimental results are provided for two different face recognition algorithms, a fingerprint recognition algorithm, a SIFT-based object recognition system, and a content-based image retrieval system.},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/3WLULYDJ/Scheirer et al. - 2011 - Meta-Recognition The Theory and Practice of Recog.pdf}
}

@article{scheirerOpenSetRecognition2013,
  title = {Toward {{Open Set Recognition}}},
  author = {Scheirer, W. J. and {de Rezende Rocha}, A. and Sapkota, A. and Boult, T. E.},
  year = {2013},
  month = jul,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {35},
  number = {7},
  pages = {1757--1772},
  issn = {0162-8828, 2160-9292},
  doi = {10.1109/TPAMI.2012.256},
  abstract = {To date, almost all experimental evaluations of machine learning-based recognition algorithms in computer vision have taken the form of ``closed set'' recognition, whereby all testing classes are known at training time. A more realistic scenario for vision applications is ``open set'' recognition, where incomplete knowledge of the world is present at training time, and unknown classes can be submitted to an algorithm during testing. This paper explores the nature of open set recognition and formalizes its definition as a constrained minimization problem. The open set recognition problem is not well addressed by existing algorithms because it requires strong generalization. As a step toward a solution, we introduce a novel ``1-vs-set machine,'' which sculpts a decision space from the marginal distances of a 1-class or binary SVM with a linear kernel. This methodology applies to several different applications in computer vision where open set recognition is a challenging problem, including object recognition and face verification. We consider both in this work, with large scale cross-dataset experiments performed over the Caltech 256 and ImageNet sets, as well as face matching experiments performed over the Labeled Faces in the Wild set. The experiments highlight the effectiveness of machines adapted for open set evaluation compared to existing 1-class and binary SVMs for the same tasks.},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/IRB2TB7P/Scheirer et al. - 2013 - Toward Open Set Recognition.pdf}
}

@article{scheirerProbabilityModelsOpen2014,
  title = {Probability {{Models}} for {{Open Set Recognition}}},
  author = {Scheirer, Walter J. and Jain, Lalit P. and Boult, Terrance E.},
  year = {2014},
  month = nov,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {36},
  number = {11},
  pages = {2317--2324},
  issn = {0162-8828, 2160-9292},
  doi = {10.1109/TPAMI.2014.2321392},
  abstract = {Real-world tasks in computer vision often touch upon open set recognition: multi-class recognition with incomplete knowledge of the world and many unknown inputs. Recent work on this problem has proposed a model incorporating an open space risk term to account for the space beyond the reasonable support of known classes. This paper extends the general idea of open space risk limiting classification to accommodate non-linear classifiers in a multiclass setting. We introduce a new open set recognition model called compact abating probability (CAP), where the probability of class membership decreases in value (abates) as points move from known data toward open space. We show that CAP models improve open set recognition for multiple algorithms. Leveraging the CAP formulation, we go on to describe the novel Weibull-calibrated SVM (W-SVM) algorithm, which combines the useful properties of statistical extreme value theory for score calibration with one-class and binary support vector machines. Our experiments show that the W-SVM is significantly better for open set object detection and OCR problems when compared to the state-of-the-art for the same tasks.},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/BFJXF338/Scheirer et al. - 2014 - Probability Models for Open Set Recognition.pdf}
}

@article{scikit-learn,
  title = {Scikit-Learn: {{Machine}} Learning in {{Python}}},
  author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  year = {2011},
  journal = {Journal of Machine Learning Research},
  volume = {12},
  pages = {2825--2830}
}

@article{searfoss2020chipper,
  title = {Chipper: {{Open-source}} Software for Semi-Automated Segmentation and Analysis of Birdsong and Other Natural Sounds},
  author = {Searfoss, Abigail M and Pino, James C and Creanza, Nicole},
  year = {2020},
  journal = {Methods in Ecology and Evolution},
  volume = {11},
  number = {4},
  pages = {524--531},
  publisher = {{Wiley Online Library}}
}

@article{simonyanVeryDeepConvolutional2015,
  title = {Very {{Deep Convolutional Networks}} for {{Large-Scale Image Recognition}}},
  author = {Simonyan, Karen and Zisserman, Andrew},
  year = {2015},
  month = apr,
  journal = {arXiv:1409.1556 [cs]},
  eprint = {1409.1556},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 \texttimes{} 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16\textendash 19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/davidnicholson/Zotero/storage/V4KBU6VJ/Simonyan and Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf}
}

@article{sober2009adult,
  title = {Adult Birdsong Is Actively Maintained by Error Correction},
  author = {Sober, Samuel J and Brainard, Michael S},
  year = {2009},
  journal = {Nature neuroscience},
  volume = {12},
  number = {7},
  pages = {927},
  publisher = {{Nature Publishing Group}}
}

@article{sober2012vocal,
  title = {Vocal Learning Is Constrained by the Statistics of Sensorimotor Experience},
  author = {Sober, Samuel J and Brainard, Michael S},
  year = {2012},
  journal = {Proceedings of the National Academy of Sciences},
  volume = {109},
  number = {51},
  pages = {21099--21103},
  publisher = {{National Acad Sciences}}
}

@article{soberCentralContributionsAcoustic2008,
  title = {Central {{Contributions}} to {{Acoustic Variation}} in {{Birdsong}}},
  author = {Sober, S. J. and Wohlgemuth, M. J. and Brainard, M. S.},
  year = {2008},
  month = oct,
  journal = {Journal of Neuroscience},
  volume = {28},
  number = {41},
  pages = {10370--10379},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2448-08.2008},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/ILING2T5/Sober et al. - 2008 - Central Contributions to Acoustic Variation in Bir.pdf}
}

@misc{songbrowser,
  title = {Song Browser},
  author = {{Troyer lab}},
  year = {2012},
  month = jul
}

@article{suthersBilateralCoordinationMotor2012,
  title = {Bilateral Coordination and the Motor Basis of Female Preference for Sexual Signals in Canary Song},
  author = {Suthers, Roderick A. and Vallet, Eric and Kreutzer, Michel},
  year = {2012},
  month = sep,
  journal = {Journal of Experimental Biology},
  volume = {215},
  number = {17},
  pages = {2950--2959},
  issn = {0022-0949},
  doi = {10.1242/jeb.071944},
  abstract = {The preference of female songbirds for particular traits in the songs of courting males has received considerable attention, but the relationship of preferred traits to male quality is poorly understood. Female domestic canaries (Serinus canaria, Linnaeus) preferentially solicit copulation with males that sing special high repetition rate, wide-band, multi-note syllables, called `sexy' or A-syllables. Syllables are separated by minibreaths but each note is produced by pulsatile expiration, allowing high repetition rates and long duration phrases. The wide bandwidth is achieved by including two notes produced sequentially on opposite sides of the syrinx, in which the left and right sides are specialized for low or high frequencies, respectively. The emphasis of low frequencies is facilitated by a positive relationship between syllable repetition rate and the bandwidth of the fundamental frequency of notes sung by the left syrinx, such that bandwidth increases with increasing syllable repetition rate. The temporal offset between notes prevents cheating by unilaterally singing a note on the left side with a low fundamental frequency and prominent higher harmonics. The syringeal and respiratory motor patterns by which sexy syllables are produced support the hypothesis that these syllables provide a sensitive vocal\textendash auditory indicator of a male's performance limit for the rapid, precisely coordinated interhemispheric switching, which is essential for many sensory and motor processes involving specialized contributions from each cerebral hemisphere.},
  file = {/Users/davidnicholson/Zotero/storage/QKPFA3WI/Suthers et al. - 2012 - Bilateral coordination and the motor basis of fema.pdf;/Users/davidnicholson/Zotero/storage/F5MMUS67/Bilateral-coordination-and-the-motor-basis-of.html}
}

@article{tachibana2014semi,
  title = {Semi-Automatic Classification of Birdsong Elements Using a Linear Support Vector Machine},
  author = {Tachibana, Ryosuke O and Oosugi, Naoya and Okanoya, Kazuo},
  year = {2014},
  journal = {PloS one},
  volume = {9},
  number = {3},
  pages = {e92584},
  publisher = {{Public Library of Science}}
}

@article{tachibanaUSVSEGRobustMethod2020,
  title = {{{USVSEG}}: {{A}} Robust Method for Segmentation of Ultrasonic Vocalizations in Rodents},
  shorttitle = {{{USVSEG}}},
  author = {Tachibana, Ryosuke O. and Kanno, Kouta and Okabe, Shota and Kobayasi, Kohta I. and Okanoya, Kazuo},
  year = {2020},
  month = feb,
  journal = {PLOS ONE},
  volume = {15},
  number = {2},
  pages = {e0228907},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0228907},
  abstract = {Rodents' ultrasonic vocalizations (USVs) provide useful information for assessing their social behaviors. Despite previous efforts in classifying subcategories of time-frequency patterns of USV syllables to study their functional relevance, methods for detecting vocal elements from continuously recorded data have remained sub-optimal. Here, we propose a novel procedure for detecting USV segments in continuous sound data containing background noise recorded during the observation of social behavior. The proposed procedure utilizes a stable version of the sound spectrogram and additional signal processing for better separation of vocal signals by reducing the variation of the background noise. Our procedure also provides precise time tracking of spectral peaks within each syllable. We demonstrated that this procedure can be applied to a variety of USVs obtained from several rodent species. Performance tests showed this method had greater accuracy in detecting USV syllables than conventional detection methods.},
  langid = {english},
  keywords = {Ambient noise,Animal behavior,Animal performance,Background signal noise,Mouse models,Rodents,Syllables,Vocalization},
  file = {/Users/davidnicholson/Zotero/storage/XJY7VCHZ/Tachibana et al. - 2020 - USVSEG A robust method for segmentation of ultras.pdf}
}

@article{takahasiStatisticalProsodicCues2010,
  title = {Statistical and Prosodic Cues for Song Segmentation Learning by {{Bengalese}} Finches ({{Lonchura}} Striata Var. Domestica)},
  author = {Takahasi, Miki and Yamada, Hiroko and Okanoya, Kazuo},
  year = {2010},
  journal = {Ethology},
  volume = {116},
  number = {6},
  pages = {481--489},
  publisher = {{Wiley Online Library}},
  file = {/Users/davidnicholson/Zotero/storage/MMNUM54H/j.1439-0310.2010.01772.html}
}

@article{tchernichovski_dynamics_2001,
  title = {Dynamics of the Vocal Imitation Process: How a Zebra Finch Learns Its Song},
  shorttitle = {Dynamics of the Vocal Imitation Process},
  author = {Tchernichovski, Ofer and Mitra, Partha P. and Lints, Thierry and Nottebohm, Fernando},
  year = {2001},
  journal = {Science (New York, N.Y.)},
  volume = {291},
  number = {5513},
  pages = {2564--2569}
}

@article{tchernichovski_procedure_2000,
  title = {A Procedure for an Automated Measurement of Song Similarity},
  author = {Tchernichovski, Ofer and Nottebohm, Fernando and Ho, Ching Elizabeth and Pesaran, Bijan and Mitra, Partha Pratim},
  year = {2000},
  journal = {Animal behaviour},
  volume = {59},
  number = {6},
  pages = {1167--1176}
}

@article{tchernichovskiDynamicsVocalImitation2001,
  title = {Dynamics of the Vocal Imitation Process: How a Zebra Finch Learns Its Song},
  shorttitle = {Dynamics of the Vocal Imitation Process},
  author = {Tchernichovski, Ofer and Mitra, Partha P. and Lints, Thierry and Nottebohm, Fernando},
  year = {2001},
  journal = {Science},
  volume = {291},
  number = {5513},
  pages = {2564--2569},
  file = {/Users/davidnicholson/Zotero/storage/DC89ZZIN/Tchernichovski et al. - 2001 - Dynamics of the vocal imitation process how a zeb.pdf;/Users/davidnicholson/Zotero/storage/GPGUHG5C/2564.html}
}

@article{tchernichovskiProcedureAutomatedMeasurement2000,
  title = {A Procedure for an Automated Measurement of Song Similarity},
  author = {Tchernichovski, Ofer and Nottebohm, Fernando and Ho, Ching Elizabeth and Pesaran, Bijan and Mitra, Partha Pratim},
  year = {2000},
  journal = {Animal behaviour},
  volume = {59},
  number = {6},
  pages = {1167--1176},
  file = {/Users/davidnicholson/Zotero/storage/868629RX/Tchernichovski et al. - 2000 - A procedure for an automated measurement of song s.pdf;/Users/davidnicholson/Zotero/storage/3RPJGAAU/S0003347299914161.html}
}

@misc{team_pandas-devpandas_2020,
  title = {Pandas-Dev/Pandas: {{Pandas}}},
  author = {pandas development {team}, The},
  year = {2020},
  month = feb,
  publisher = {{Zenodo}},
  doi = {10.5281/zenodo.3509134}
}

@misc{teamPandasdevPandasPandas2020,
  title = {Pandas-Dev/Pandas: {{Pandas}}},
  author = {pandas development {team}, The},
  year = {2020},
  month = feb,
  doi = {10.5281/zenodo.3509134},
  howpublished = {Zenodo}
}

@article{tensorflow2015-whitepaper,
  title = {{{TensorFlow}}: {{Large-Scale Machine Learning}} on {{Heterogeneous Systems}}},
  author = {Abadi, Mar{\'t}{\i}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Man{\'e}, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Vi{\'e}gas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
  year = {2015}
}

@article{tensorflow2015-whitepaper,
  title = {{{TensorFlow}}: {{Large-Scale Machine Learning}} on {{Heterogeneous Systems}}},
  author = {Abadi, Mar{\'t}{\i}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Man{\'e}, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Vi{\'e}gas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
  year = {2015}
}

@misc{thomas_a_caswell_2020_4030140,
  title = {Matplotlib/Matplotlib: {{REL}}: V3.3.2},
  author = {Caswell, Thomas A and Droettboom, Michael and Lee, Antony and Hunter, John and {de Andrade}, Elliott Sales and Firing, Eric and Hoffmann, Tim and Klymak, Jody and Stansby, David and Varoquaux, Nelle and Nielsen, Jens Hedegaard and Root, Benjamin and May, Ryan and Elson, Phil and Sepp{\"a}nen, Jouni K. and Dale, Darren and Lee, Jae-Joon and McDougall, Damon and Straw, Andrew and Hobson, Paul and Gohlke, Christoph and Yu, Tony S and Ma, Eric and Vincent, Adrien F. and Silvester, Steven and Moad, Charlie and Kniazev, Nikita and {hannah} and Ernest, Elan and Ivanov, Paul},
  year = {2020},
  month = sep,
  doi = {10.5281/zenodo.4030140},
  howpublished = {Zenodo}
}

@article{thompson1994system,
  title = {A System for Describing Bird Song Units},
  author = {Thompson, Nicholas S and LeDoux, Kerry and Moody, Kevin},
  year = {1994},
  journal = {Bioacoustics-the International Journal of Animal Sound and Its Recording},
  volume = {5},
  number = {4},
  pages = {267--279},
  publisher = {{Taylor \& Francis}},
  file = {/Users/davidnicholson/Zotero/storage/BS3LAMK7/Thompson et al. - 1994 - A system for describing bird song units.pdf}
}

@article{tumer_performance_2007,
  title = {Performance Variability Enables Adaptive Plasticity of `Crystallized' Adult Birdsong},
  author = {Tumer, Evren C. and Brainard, Michael S.},
  year = {2007},
  month = dec,
  journal = {Nature},
  volume = {450},
  number = {7173},
  pages = {1240--1244},
  issn = {1476-4687},
  doi = {10.1038/nature06390},
  abstract = {Why is it that even the best-trained athletes and musicians cannot perform perfectly? One thought is that residual variability in performance is 'noise' that reflects fundamental limits on our ability to control our movements. Experiments using the exceptionally well-rehearsed songs of adult songbirds as a model point to an alternative explanation. Computerized monitoring of the apparently stereotyped songs of adult Bengalese finches revealed minuscule variations in performance. When the birds were given corrections each time the song varied beyond a certain limit, they rapidly learned to adapt their vocalizations. The implication is that once learned, songs can be maintained despite subtle changes to the vocal system due to factors such as ageing. So behavioural 'noise', rather than simply being a nuisance, may reflect experimentation by the nervous system to refine performance.},
  copyright = {2007 Nature Publishing Group},
  langid = {english}
}

@article{tumerPerformanceVariabilityEnables2007,
  title = {Performance Variability Enables Adaptive Plasticity of `Crystallized' Adult Birdsong},
  author = {Tumer, Evren C. and Brainard, Michael S.},
  year = {2007},
  month = dec,
  journal = {Nature},
  volume = {450},
  number = {7173},
  pages = {1240--1244},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature06390},
  abstract = {Why is it that even the best-trained athletes and musicians cannot perform perfectly? One thought is that residual variability in performance is 'noise' that reflects fundamental limits on our ability to control our movements. Experiments using the exceptionally well-rehearsed songs of adult songbirds as a model point to an alternative explanation. Computerized monitoring of the apparently stereotyped songs of adult Bengalese finches revealed minuscule variations in performance. When the birds were given corrections each time the song varied beyond a certain limit, they rapidly learned to adapt their vocalizations. The implication is that once learned, songs can be maintained despite subtle changes to the vocal system due to factors such as ageing. So behavioural 'noise', rather than simply being a nuisance, may reflect experimentation by the nervous system to refine performance.},
  copyright = {2007 Nature Publishing Group},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/MG7SHEQL/Tumer and Brainard - 2007 - Performance variability enables adaptive plasticit.pdf;/Users/davidnicholson/Zotero/storage/9LIXENH6/nature06390.html}
}

@article{vaswani_attention_2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = dec,
  journal = {arXiv:1706.03762 [cs]},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv}
}

@article{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = dec,
  journal = {arXiv:1706.03762 [cs]},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  file = {/Users/davidnicholson/Zotero/storage/U43422UR/Vaswani et al. - 2017 - Attention Is All You Need.pdf;/Users/davidnicholson/Zotero/storage/65WKBFLE/1706.html}
}

@article{veit2021songbirds,
  title = {Songbirds Can Learn Flexible Contextual Control over Syllable Sequencing},
  author = {Veit, Lena and Tian, Lucas Y and Hernandez, Christian J Monroy and Brainard, Michael S},
  year = {2021},
  journal = {Elife},
  volume = {10},
  pages = {e61610},
  publisher = {{eLife Sciences Publications Limited}}
}

@article{virtanen_scipy_2019,
  title = {{{SciPy}} 1.0\textendash{{Fundamental Algorithms}} for {{Scientific Computing}} in {{Python}}},
  author = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and {van der Walt}, St{\'e}fan J. and Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, C. J. and Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and VanderPlas, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and Harris, Charles R. and Archibald, Anne M. and Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and {van Mulbregt}, Paul and Contributors, SciPy 1 0},
  year = {2019},
  month = jul,
  journal = {arXiv:1907.10121 [physics]},
  eprint = {1907.10121},
  eprinttype = {arxiv},
  primaryclass = {physics},
  abstract = {SciPy is an open source scientific computing library for the Python programming language. SciPy 1.0 was released in late 2017, about 16 years after the original version 0.1 release. SciPy has become a de facto standard for leveraging scientific algorithms in the Python programming language, with more than 600 unique code contributors, thousands of dependent packages, over 100,000 dependent repositories, and millions of downloads per year. This includes usage of SciPy in almost half of all machine learning projects on GitHub, and usage by high profile projects including LIGO gravitational wave analysis and creation of the first-ever image of a black hole (M87). The library includes functionality spanning clustering, Fourier transforms, integration, interpolation, file I/O, linear algebra, image processing, orthogonal distance regression, minimization algorithms, signal processing, sparse matrix handling, computational geometry, and statistics. In this work, we provide an overview of the capabilities and development practices of the SciPy library and highlight some recent technical developments.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Mathematical Software,Computer Science - Software Engineering,Physics - Computational Physics}
}

@article{virtanen_scipy_2019,
  title = {{{SciPy}} 1.0\textendash{{Fundamental Algorithms}} for {{Scientific Computing}} in {{Python}}},
  author = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and {van der Walt}, St{\'e}fan J. and Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, C. J. and Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and VanderPlas, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and Harris, Charles R. and Archibald, Anne M. and Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and {van Mulbregt}, Paul and Contributors, SciPy 1 0},
  year = {2019},
  month = jul,
  journal = {arXiv:1907.10121 [physics]},
  eprint = {1907.10121},
  eprinttype = {arxiv},
  primaryclass = {physics},
  abstract = {SciPy is an open source scientific computing library for the Python programming language. SciPy 1.0 was released in late 2017, about 16 years after the original version 0.1 release. SciPy has become a de facto standard for leveraging scientific algorithms in the Python programming language, with more than 600 unique code contributors, thousands of dependent packages, over 100,000 dependent repositories, and millions of downloads per year. This includes usage of SciPy in almost half of all machine learning projects on GitHub, and usage by high profile projects including LIGO gravitational wave analysis and creation of the first-ever image of a black hole (M87). The library includes functionality spanning clustering, Fourier transforms, integration, interpolation, file I/O, linear algebra, image processing, orthogonal distance regression, minimization algorithms, signal processing, sparse matrix handling, computational geometry, and statistics. In this work, we provide an overview of the capabilities and development practices of the SciPy library and highlight some recent technical developments.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Mathematical Software,Computer Science - Software Engineering,Physics - Computational Physics}
}

@article{virtanen_scipy_2019,
  title = {{{SciPy}} 1.0\textendash{{Fundamental Algorithms}} for {{Scientific Computing}} in {{Python}}},
  author = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and {van der Walt}, St{\'e}fan J. and Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, C. J. and Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and VanderPlas, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and Harris, Charles R. and Archibald, Anne M. and Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and {van Mulbregt}, Paul and Contributors, SciPy 1 0},
  year = {2019},
  month = jul,
  journal = {arXiv:1907.10121 [physics]},
  eprint = {1907.10121},
  eprinttype = {arxiv},
  primaryclass = {physics},
  abstract = {SciPy is an open source scientific computing library for the Python programming language. SciPy 1.0 was released in late 2017, about 16 years after the original version 0.1 release. SciPy has become a de facto standard for leveraging scientific algorithms in the Python programming language, with more than 600 unique code contributors, thousands of dependent packages, over 100,000 dependent repositories, and millions of downloads per year. This includes usage of SciPy in almost half of all machine learning projects on GitHub, and usage by high profile projects including LIGO gravitational wave analysis and creation of the first-ever image of a black hole (M87). The library includes functionality spanning clustering, Fourier transforms, integration, interpolation, file I/O, linear algebra, image processing, orthogonal distance regression, minimization algorithms, signal processing, sparse matrix handling, computational geometry, and statistics. In this work, we provide an overview of the capabilities and development practices of the SciPy library and highlight some recent technical developments.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Mathematical Software,Computer Science - Software Engineering,Physics - Computational Physics}
}

@article{virtanen_scipy_2020,
  title = {{{SciPy}} 1.0: Fundamental Algorithms for Scientific Computing in {{Python}}},
  shorttitle = {{{SciPy}} 1.0},
  author = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and {van der Walt}, St{\'e}fan J. and Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, C. J. and Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and VanderPlas, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and Harris, Charles R. and Archibald, Anne M. and Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and {van Mulbregt}, Paul},
  year = {2020},
  month = mar,
  journal = {Nature Methods},
  volume = {17},
  number = {3},
  pages = {261--272},
  issn = {1548-7105},
  doi = {10.1038/s41592-019-0686-2},
  abstract = {SciPy is an open-source scientific computing library for the Python programming language. Since its initial release in 2001, SciPy has become a de facto standard for leveraging scientific algorithms in Python, with over 600 unique code contributors, thousands of dependent packages, over 100,000 dependent repositories and millions of downloads per year. In this work, we provide an overview of the capabilities and development practices of SciPy 1.0 and highlight some recent technical developments.},
  copyright = {2020 The Author(s)},
  langid = {english}
}

@article{virtanenSciPyFundamentalAlgorithms2019,
  title = {{{SciPy}} 1.0--{{Fundamental Algorithms}} for {{Scientific Computing}} in {{Python}}},
  author = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and {van der Walt}, St{\'e}fan J. and Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, C. J. and Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and VanderPlas, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and Harris, Charles R. and Archibald, Anne M. and Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and {van Mulbregt}, Paul and Contributors, SciPy 1 0},
  year = {2019},
  month = jul,
  journal = {arXiv:1907.10121 [physics]},
  eprint = {1907.10121},
  eprinttype = {arxiv},
  primaryclass = {physics},
  abstract = {SciPy is an open source scientific computing library for the Python programming language. SciPy 1.0 was released in late 2017, about 16 years after the original version 0.1 release. SciPy has become a de facto standard for leveraging scientific algorithms in the Python programming language, with more than 600 unique code contributors, thousands of dependent packages, over 100,000 dependent repositories, and millions of downloads per year. This includes usage of SciPy in almost half of all machine learning projects on GitHub, and usage by high profile projects including LIGO gravitational wave analysis and creation of the first-ever image of a black hole (M87). The library includes functionality spanning clustering, Fourier transforms, integration, interpolation, file I/O, linear algebra, image processing, orthogonal distance regression, minimization algorithms, signal processing, sparse matrix handling, computational geometry, and statistics. In this work, we provide an overview of the capabilities and development practices of the SciPy library and highlight some recent technical developments.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Mathematical Software,Computer Science - Software Engineering,Physics - Computational Physics},
  file = {/Users/davidnicholson/Zotero/storage/YLK3BXWU/Virtanen et al. - 2019 - SciPy 1.0--Fundamental Algorithms for Scientific C.pdf;/Users/davidnicholson/Zotero/storage/XEGKFVEW/1907.html}
}

@article{virtanenSciPyFundamentalAlgorithms2020,
  title = {{{SciPy}} 1.0: Fundamental Algorithms for Scientific Computing in {{Python}}},
  shorttitle = {{{SciPy}} 1.0},
  author = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and {van der Walt}, St{\'e}fan J. and Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, C. J. and Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and VanderPlas, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and Harris, Charles R. and Archibald, Anne M. and Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and {van Mulbregt}, Paul},
  year = {2020},
  month = mar,
  journal = {Nature Methods},
  volume = {17},
  number = {3},
  pages = {261--272},
  publisher = {{Nature Publishing Group}},
  issn = {1548-7105},
  doi = {10.1038/s41592-019-0686-2},
  abstract = {SciPy is an open-source scientific computing library for the Python programming language. Since its initial release in 2001, SciPy has become a de facto standard for leveraging scientific algorithms in Python, with over 600 unique code contributors, thousands of dependent packages, over 100,000 dependent repositories and millions of downloads per year. In this work, we provide an overview of the capabilities and development practices of SciPy 1.0 and highlight some recent technical developments.},
  copyright = {2020 The Author(s)},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/DGRDR5GN/Virtanen et al. - 2020 - SciPy 1.0 fundamental algorithms for scientific c.pdf;/Users/davidnicholson/Zotero/storage/H82QAU5F/s41592-019-0686-2.html}
}

@article{walt_numpy_2011,
  title = {The {{NumPy Array}}: {{A Structure}} for {{Efficient Numerical Computation}}},
  shorttitle = {The {{NumPy Array}}},
  author = {van der Walt, S. and Colbert, S. C. and Varoquaux, G.},
  year = {2011},
  month = mar,
  journal = {Computing in Science Engineering},
  volume = {13},
  number = {2},
  pages = {22--30},
  issn = {1521-9615},
  doi = {10.1109/MCSE.2011.37},
  abstract = {In the Python world, NumPy arrays are the standard representation for numerical data and enable efficient implementation of numerical computations in a high-level language. As this effort shows, NumPy performance can be improved through three techniques: vectorizing calculations, avoiding copying data in memory, and minimizing operation counts.},
  keywords = {Arrays,Computational efficiency,data structures,Finite element methods,high level language,high level languages,mathematics computing,numerical analysis,Numerical analysis,numerical computation,numerical computations,numerical data,NumPy,numpy array,Performance evaluation,programming libraries,Python,Python programming language,Resource management,scientific programming,Vector quantization}
}

@article{waltNumPyArrayStructure2011,
  ids = {walt_numpy_2011},
  title = {The {{NumPy Array}}: {{A Structure}} for {{Efficient Numerical Computation}}},
  shorttitle = {The {{NumPy Array}}},
  author = {van der Walt, S. and Colbert, S. C. and Varoquaux, G.},
  year = {2011},
  month = mar,
  journal = {Computing in Science Engineering},
  volume = {13},
  number = {2},
  pages = {22--30},
  issn = {1521-9615},
  doi = {10.1109/MCSE.2011.37},
  abstract = {In the Python world, NumPy arrays are the standard representation for numerical data and enable efficient implementation of numerical computations in a high-level language. As this effort shows, NumPy performance can be improved through three techniques: vectorizing calculations, avoiding copying data in memory, and minimizing operation counts.},
  keywords = {Arrays,Computational efficiency,data structures,Finite element methods,high level language,high level languages,mathematics computing,numerical analysis,Numerical analysis,numerical computation,numerical computations,numerical data,NumPy,numpy array,Performance evaluation,programming libraries,Python,Python programming language,Resource management,scientific programming,Vector quantization},
  file = {/Users/davidnicholson/Zotero/storage/F3I4NFFR/5725236.html}
}

@article{warren_variable_2012,
  ids = {warrenVariableSequencingActively2012},
  title = {Variable {{Sequencing Is Actively Maintained}} in a {{Well Learned Motor Skill}}},
  author = {Warren, Timothy L. and Charlesworth, Jonathan D. and Tumer, Evren C. and Brainard, Michael S.},
  year = {2012},
  month = oct,
  journal = {The Journal of Neuroscience},
  volume = {32},
  number = {44},
  pages = {15414--15425},
  issn = {0270-6474},
  doi = {10.1523/JNEUROSCI.1254-12.2012},
  abstract = {Variation in sequencing of actions occurs in many natural behaviors, yet how such variation is maintained is poorly understood. We investigated maintenance of sequence variation in adult Bengalese finch song, a learned skill with rendition-to-rendition variation in the sequencing of discrete syllables (i.e., syllable ``b'' might transition to ``c'' with 70\% probability and to ``d'' with 30\% probability). We found that probabilities of transitions ordinarily remain stable but could be modified by delivering aversive noise bursts following one transition (e.g., ``b\textrightarrow c'') but not the alternative (e.g., ``b\textrightarrow d''). Such differential reinforcement induced gradual, adaptive decreases in probabilities of targeted transitions and compensatory increases in alternative transitions. Thus, the normal stability of transition probabilities does not reflect hardwired premotor circuitry. While all variable transitions could be modified by differential reinforcement, some were less readily modified than others; these were cases that exhibited more alternation between possible transitions than predicted by chance (i.e., ``b\textrightarrow d '' would tend to follow ``b\textrightarrow c '' and vice versa). These history-dependent transitions were less modifiable than more stochastic transitions. Similarly, highly stereotyped transitions (which are completely predictable) were not modifiable. This suggests that stochastically generated variability is crucial for sequence modification. Finally, we found that, when reinforcement ceased, birds gradually restored transition probabilities to their baseline values. Hence, the nervous system retains a representation of baseline probabilities and has the impetus to restore them. Together, our results indicate that variable sequencing in a motor skill can reflect an end point of learning that is stably maintained via continual self-monitoring.},
  pmcid = {PMC3752123},
  pmid = {23115179}
}

@article{warren_variable_2012,
  title = {Variable {{Sequencing Is Actively Maintained}} in a {{Well Learned Motor Skill}}},
  author = {Warren, Timothy L. and Charlesworth, Jonathan D. and Tumer, Evren C. and Brainard, Michael S.},
  year = {2012},
  month = oct,
  journal = {The Journal of Neuroscience},
  volume = {32},
  number = {44},
  pages = {15414--15425},
  issn = {0270-6474},
  doi = {10.1523/JNEUROSCI.1254-12.2012},
  abstract = {Variation in sequencing of actions occurs in many natural behaviors, yet how such variation is maintained is poorly understood. We investigated maintenance of sequence variation in adult Bengalese finch song, a learned skill with rendition-to-rendition variation in the sequencing of discrete syllables (i.e., syllable ``b'' might transition to ``c'' with 70\% probability and to ``d'' with 30\% probability). We found that probabilities of transitions ordinarily remain stable but could be modified by delivering aversive noise bursts following one transition (e.g., ``b\textrightarrow c'') but not the alternative (e.g., ``b\textrightarrow d''). Such differential reinforcement induced gradual, adaptive decreases in probabilities of targeted transitions and compensatory increases in alternative transitions. Thus, the normal stability of transition probabilities does not reflect hardwired premotor circuitry. While all variable transitions could be modified by differential reinforcement, some were less readily modified than others; these were cases that exhibited more alternation between possible transitions than predicted by chance (i.e., ``b\textrightarrow d '' would tend to follow ``b\textrightarrow c '' and vice versa). These history-dependent transitions were less modifiable than more stochastic transitions. Similarly, highly stereotyped transitions (which are completely predictable) were not modifiable. This suggests that stochastically generated variability is crucial for sequence modification. Finally, we found that, when reinforcement ceased, birds gradually restored transition probabilities to their baseline values. Hence, the nervous system retains a representation of baseline probabilities and has the impetus to restore them. Together, our results indicate that variable sequencing in a motor skill can reflect an end point of learning that is stably maintained via continual self-monitoring.},
  pmcid = {PMC3752123},
  pmid = {23115179}
}

@article{Waskom2021,
  title = {Seaborn: Statistical Data Visualization},
  author = {Waskom, Michael L.},
  year = {2021},
  journal = {Journal of Open Source Software},
  volume = {6},
  number = {60},
  pages = {3021},
  publisher = {{The Open Journal}},
  doi = {10.21105/joss.03021}
}

@techreport{weertsPsychometricsAutomaticSpeech2021,
  type = {Preprint},
  title = {The {{Psychometrics}} of {{Automatic Speech Recognition}}},
  author = {Weerts, Lotte and Rosen, Stuart and Clopath, Claudia and Goodman, Dan F. M.},
  year = {2021},
  month = apr,
  institution = {{Neuroscience}},
  doi = {10.1101/2021.04.19.440438},
  abstract = {Automatic speech recognition (ASR) software has been suggested as a candidate model of the human auditory system thanks to recent dramatic improvements in performance. To test this hypothesis, we compared several state-of-the-art ASR systems to results from humans on a barrage of standard psychometric experiments. While some systems showed qualitative agreement with humans in certain tests, in others all tested systems diverged markedly from humans. In particular, all systems used spectral invariance, temporal fine structure and speech periodicity differently from humans. We conclude that none of the tested ASR systems can yet act as a strong proxy for human speech recognition. However, we note that the more recent systems with better performance also tend to better match human results, suggesting that continued cross-fertilisation of ideas between human and automatic speech recognition may be fruitful. Our open source toolbox allows researchers to assess future ASR systems or add additional psychoacoustic measures.},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/4HFM5YJN/Weerts et al. - 2021 - The Psychometrics of Automatic Speech Recognition.pdf}
}

@article{wilbrechtNeuronAdditionLoss2004,
  title = {Neuron {{Addition}} and {{Loss}} in the {{Song System}}: {{Regulation}} and {{Function}}},
  shorttitle = {Neuron {{Addition}} and {{Loss}} in the {{Song System}}},
  author = {Wilbrecht, Linda and Kirn, John R.},
  year = {2004},
  journal = {Annals of the New York Academy of Sciences},
  volume = {1016},
  number = {1},
  pages = {659--683},
  issn = {1749-6632},
  doi = {10.1196/annals.1298.024},
  abstract = {Abstract: Neurons continue to be produced and replaced throughout life in songbirds. Proliferation in the walls of the lateral ventricles gives rise to neurons that migrate long distances to populate many diverse telencephalic regions, including nuclei dedicated to the perception and production of song, a learned behavior. Many projection neurons are incorporated into the efferent motor pathway for song control. Replacement of these neurons is regulated, in part, by neuron death. Underlying mechanisms include gonadal steroids and BDNF, but are likely to involve other trophic factors as well. The functional significance of neuronal replacement remains unclear. However, recent experiments suggest a link between cell turnover and one or more specific attributes of song learning and production. Several hypotheses are critically examined, including the possibility that neuronal replacement provides motor flexibility to allow for error correction\textemdash a capacity needed for juvenile and adult song learning, but also likely to be important for the maintenance of song stereotypy. We highlight important gaps in our knowledge and discuss future directions that may bring us closer to solving the riddle of why neurons are produced and replaced in adulthood.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1196/annals.1298.024},
  file = {/Users/davidnicholson/Zotero/storage/HWP939A3/Wilbrecht and Kirn - 2004 - Neuron Addition and Loss in the Song System Regul.pdf;/Users/davidnicholson/Zotero/storage/N9KFPCZI/annals.1298.html}
}

@article{wittenbach_adapting_2015,
  ids = {wittenbachAdaptingAuditorymotorFeedback2015},
  title = {An {{Adapting Auditory-motor Feedback Loop Can Contribute}} to {{Generating Vocal Repetition}}},
  author = {Wittenbach, Jason D. and Bouchard, Kristofer E. and Brainard, Michael S. and Jin, Dezhe Z.},
  year = {2015},
  month = oct,
  journal = {PLoS computational biology},
  volume = {11},
  number = {10},
  pages = {e1004471},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004471},
  abstract = {Consecutive repetition of actions is common in behavioral sequences. Although integration of sensory feedback with internal motor programs is important for sequence generation, if and how feedback contributes to repetitive actions is poorly understood. Here we study how auditory feedback contributes to generating repetitive syllable sequences in songbirds. We propose that auditory signals provide positive feedback to ongoing motor commands, but this influence decays as feedback weakens from response adaptation during syllable repetitions. Computational models show that this mechanism explains repeat distributions observed in Bengalese finch song. We experimentally confirmed two predictions of this mechanism in Bengalese finches: removal of auditory feedback by deafening reduces syllable repetitions; and neural responses to auditory playback of repeated syllable sequences gradually adapt in sensory-motor nucleus HVC. Together, our results implicate a positive auditory-feedback loop with adaptation in generating repetitive vocalizations, and suggest sensory adaptation is important for feedback control of motor sequences.},
  langid = {english},
  pmcid = {PMC4598084},
  pmid = {26448054},
  keywords = {Adaptation,Adaptation; Physiological,Animal,Animals,Auditory Cortex,Auditory Pathways,Computer Simulation,Efferent Pathways,Feedback,Feedback; Physiological,Male,Models,Models; Neurological,Motor Cortex,Movement,Neurological,Physiological,Songbirds,Vocalization,Vocalization; Animal}
}

@article{wittenbach_adapting_2015,
  title = {An {{Adapting Auditory-motor Feedback Loop Can Contribute}} to {{Generating Vocal Repetition}}},
  author = {Wittenbach, Jason D. and Bouchard, Kristofer E. and Brainard, Michael S. and Jin, Dezhe Z.},
  year = {2015},
  month = oct,
  journal = {PLoS computational biology},
  volume = {11},
  number = {10},
  pages = {e1004471},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004471},
  abstract = {Consecutive repetition of actions is common in behavioral sequences. Although integration of sensory feedback with internal motor programs is important for sequence generation, if and how feedback contributes to repetitive actions is poorly understood. Here we study how auditory feedback contributes to generating repetitive syllable sequences in songbirds. We propose that auditory signals provide positive feedback to ongoing motor commands, but this influence decays as feedback weakens from response adaptation during syllable repetitions. Computational models show that this mechanism explains repeat distributions observed in Bengalese finch song. We experimentally confirmed two predictions of this mechanism in Bengalese finches: removal of auditory feedback by deafening reduces syllable repetitions; and neural responses to auditory playback of repeated syllable sequences gradually adapt in sensory-motor nucleus HVC. Together, our results implicate a positive auditory-feedback loop with adaptation in generating repetitive vocalizations, and suggest sensory adaptation is important for feedback control of motor sequences.},
  langid = {english},
  pmcid = {PMC4598084},
  pmid = {26448054},
  keywords = {Adaptation,Animal,Animals,Auditory Cortex,Auditory Pathways,Computer Simulation,Efferent Pathways,Feedback,Male,Models,Motor Cortex,Movement,Neurological,Physiological,Songbirds,Vocalization}
}

@article{wohlgemuth_linked_2010,
  ids = {wohlgemuthLinkedControlSyllable2010},
  title = {Linked {{Control}} of {{Syllable Sequence}} and {{Phonology}} in {{Birdsong}}},
  author = {Wohlgemuth, Melville J. and Sober, Samuel J. and Brainard, Michael S.},
  year = {2010},
  month = sep,
  journal = {Journal of Neuroscience},
  volume = {30},
  number = {39},
  pages = {12936--12949},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2690-10.2010},
  abstract = {The control of sequenced behaviors, including human speech, requires that the brain coordinate the production of discrete motor elements with their concatenation into complex patterns. In birdsong, another sequential vocal behavior, the acoustic structure (phonology) of individual song elements, or ``syllables,'' must be coordinated with the sequencing of syllables into a song. However, it is unknown whether syllable phonology is independent of the sequence in which a syllable is produced. We quantified interactions between phonology and sequence in Bengalese finch song by examining both convergent syllables, which can be preceded by at least two different syllables and divergent syllables, which can be followed by at least two different syllables. Phonology differed significantly based on the identity of the preceding syllable for 97\% of convergent syllables and differed significantly with the identity of the upcoming syllable for 92\% of divergent syllables. Furthermore, sequence-dependent phonological differences extended at least two syllables away from the convergent or divergent syllable. To determine whether these phenomena reflect differences in central control, we analyzed premotor neural activity in the robust nucleus of the arcopallium (RA). Activity associated with a syllable varied significantly depending on the sequence in which the syllable was produced, suggesting that sequence-dependent variations in premotor activity contribute to sequence-dependent differences in phonology. Moreover, these data indicate that RA activity could contribute to the sequencing of syllables. Together, these results suggest that, rather than being controlled independently, the sequence and phonology of birdsong are intimately related, as is the case for human speech.},
  copyright = {Copyright \textcopyright{} 2010 the authors 0270-6474/10/3012936-14\$15.00/0},
  langid = {english},
  pmid = {20881112},
  file = {/Users/davidnicholson/Zotero/storage/LI48HV76/Wohlgemuth et al. - 2010 - Linked Control of Syllable Sequence and Phonology .pdf;/Users/davidnicholson/Zotero/storage/B5XCH63M/12936.html}
}

@article{wohlgemuth_linked_2010,
  title = {Linked {{Control}} of {{Syllable Sequence}} and {{Phonology}} in {{Birdsong}}},
  author = {Wohlgemuth, Melville J. and Sober, Samuel J. and Brainard, Michael S.},
  year = {2010},
  month = sep,
  journal = {Journal of Neuroscience},
  volume = {30},
  number = {39},
  pages = {12936--12949},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2690-10.2010},
  abstract = {The control of sequenced behaviors, including human speech, requires that the brain coordinate the production of discrete motor elements with their concatenation into complex patterns. In birdsong, another sequential vocal behavior, the acoustic structure (phonology) of individual song elements, or ``syllables,'' must be coordinated with the sequencing of syllables into a song. However, it is unknown whether syllable phonology is independent of the sequence in which a syllable is produced. We quantified interactions between phonology and sequence in Bengalese finch song by examining both convergent syllables, which can be preceded by at least two different syllables and divergent syllables, which can be followed by at least two different syllables. Phonology differed significantly based on the identity of the preceding syllable for 97\% of convergent syllables and differed significantly with the identity of the upcoming syllable for 92\% of divergent syllables. Furthermore, sequence-dependent phonological differences extended at least two syllables away from the convergent or divergent syllable. To determine whether these phenomena reflect differences in central control, we analyzed premotor neural activity in the robust nucleus of the arcopallium (RA). Activity associated with a syllable varied significantly depending on the sequence in which the syllable was produced, suggesting that sequence-dependent variations in premotor activity contribute to sequence-dependent differences in phonology. Moreover, these data indicate that RA activity could contribute to the sequencing of syllables. Together, these results suggest that, rather than being controlled independently, the sequence and phonology of birdsong are intimately related, as is the case for human speech.},
  langid = {english},
  pmid = {20881112}
}

@article{wohlgemuth2010linked,
  title = {Linked Control of Syllable Sequence and Phonology in Birdsong},
  author = {Wohlgemuth, Melville J and Sober, Samuel J and Brainard, Michael S},
  year = {2010},
  journal = {Journal of Neuroscience},
  volume = {30},
  number = {39},
  pages = {12936--12949},
  publisher = {{Soc Neuroscience}}
}

@article{yamahachi_undirected_2020,
  title = {Undirected Singing Rate as a Non-Invasive Tool for Welfare Monitoring in Isolated Male Zebra Finches},
  author = {Yamahachi, Homare and Zai, Anja T. and Tachibana, Ryosuke O. and Stepien, Anna E. and Rodrigues, Diana I. and {Cav{\'e}-Lopez}, Sophie and Lorenz, Corinna and Arneodo, Ezequiel M. and Giret, Nicolas and Hahnloser, Richard H. R.},
  year = {2020},
  month = aug,
  journal = {PLOS ONE},
  volume = {15},
  number = {8},
  pages = {e0236333},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0236333},
  abstract = {Research on the songbird zebra finch (Taeniopygia guttata) has advanced our behavioral, hormonal, neuronal, and genetic understanding of vocal learning. However, little is known about the impact of typical experimental manipulations on the welfare of these birds. Here we explore whether the undirected singing rate can be used as an indicator of welfare. We tested this idea by performing a post hoc analysis of singing behavior in isolated male zebra finches subjected to interactive white noise, to surgery, or to tethering. We find that the latter two experimental manipulations transiently but reliably decreased singing rates. By contraposition, we infer that a high-sustained singing rate is suggestive of successful coping or improved welfare in these experiments. Our analysis across more than 300 days of song data suggests that a singing rate above a threshold of several hundred song motifs per day implies an absence of an acute stressor or a successful coping with stress. Because singing rate can be measured in a completely automatic fashion, its observation can help to reduce experimenter bias in welfare monitoring. Because singing rate measurements are non-invasive, we expect this study to contribute to the refinement of the current welfare monitoring tools in zebra finches.},
  langid = {english}
}

@article{yamahachiUndirectedSingingRate2020,
  title = {Undirected Singing Rate as a Non-Invasive Tool for Welfare Monitoring in Isolated Male Zebra Finches},
  author = {Yamahachi, Homare and Zai, Anja T. and Tachibana, Ryosuke O. and Stepien, Anna E. and Rodrigues, Diana I. and {Cav{\'e}-Lopez}, Sophie and Lorenz, Corinna and Arneodo, Ezequiel M. and Giret, Nicolas and Hahnloser, Richard H. R.},
  year = {2020},
  month = aug,
  journal = {PLOS ONE},
  volume = {15},
  number = {8},
  pages = {e0236333},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0236333},
  abstract = {Research on the songbird zebra finch (Taeniopygia guttata) has advanced our behavioral, hormonal, neuronal, and genetic understanding of vocal learning. However, little is known about the impact of typical experimental manipulations on the welfare of these birds. Here we explore whether the undirected singing rate can be used as an indicator of welfare. We tested this idea by performing a post hoc analysis of singing behavior in isolated male zebra finches subjected to interactive white noise, to surgery, or to tethering. We find that the latter two experimental manipulations transiently but reliably decreased singing rates. By contraposition, we infer that a high-sustained singing rate is suggestive of successful coping or improved welfare in these experiments. Our analysis across more than 300 days of song data suggests that a singing rate above a threshold of several hundred song motifs per day implies an absence of an acute stressor or a successful coping with stress. Because singing rate can be measured in a completely automatic fashion, its observation can help to reduce experimenter bias in welfare monitoring. Because singing rate measurements are non-invasive, we expect this study to contribute to the refinement of the current welfare monitoring tools in zebra finches.},
  langid = {english},
  file = {/Users/davidnicholson/Zotero/storage/DUXQDEHI/Yamahachi et al. - 2020 - Undirected singing rate as a non-invasive tool for.pdf}
}

@article{yardencsgithub_hybrid_2019,
  title = {Hybrid Convolutional-Recurrent Neural Networks for Segmentation of Birdsong and Classification of Elements: {{yardencsGitHub}}/Tweetynet},
  shorttitle = {Hybrid Convolutional-Recurrent Neural Networks for Segmentation of Birdsong and Classification of Elements},
  author = {{yardencsGitHub}},
  year = {2019},
  month = aug,
  copyright = {BSD-3-Clause}
}

@misc{yardencsgithub_hybrid_2019,
  title = {Hybrid Convolutional-Recurrent Neural Networks for Segmentation of Birdsong and Classification of Elements: {{yardencsGitHub}}/Tweetynet},
  shorttitle = {Hybrid Convolutional-Recurrent Neural Networks for Segmentation of Birdsong and Classification of Elements},
  author = {{yardencsGitHub}},
  year = {2019},
  month = aug,
  copyright = {BSD-3-Clause}
}

@article{yardencsgithub_hybrid_2019,
  title = {Hybrid Convolutional-Recurrent Neural Networks for Segmentation of Birdsong and Classification of Elements: {{yardencsGitHub}}/Tweetynet},
  shorttitle = {Hybrid Convolutional-Recurrent Neural Networks for Segmentation of Birdsong and Classification of Elements},
  author = {{yardencsGitHub}},
  year = {2019},
  month = aug
}

@misc{yardencsgithubHybridConvolutionalrecurrentNeural2019,
  title = {Hybrid Convolutional-Recurrent Neural Networks for Segmentation of Birdsong and Classification of Elements: {{yardencsGitHub}}/Tweetynet},
  shorttitle = {Hybrid Convolutional-Recurrent Neural Networks for Segmentation of Birdsong and Classification of Elements},
  author = {{yardencsGitHub}},
  year = {2019},
  month = aug,
  copyright = {BSD-3-Clause}
}


